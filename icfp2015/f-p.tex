%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{listings,xspace}
\usepackage{amsthm}
\usepackage{mdframed}
\usepackage{xcolor}
\usepackage{fontspec}
\usepackage{graphicx}
\usepackage{setspace}
% \usepackage{booktabs}
\usepackage{wasysym}
\usepackage{amsthm}
\usepackage{url}

\usepackage{amssymb}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{todonotes}

\usepackage{bcprules}
\usepackage{prooftree}
\usepackage{multicol}
\usepackage{mathpartir}

\lstdefinelanguage{Scala}%
{morekeywords={abstract,case,catch,char,class,%
    def,else,extends,final,%
    if,import,%
    match,module,new,null,object,override,package,private,protected,%
    public,return,super,this,throw,trait,try,type,val,var,with,implicit,%
    macro,sealed,%
  },%
  sensitive,%
  morecomment=[l]//,%
  morecomment=[s]{/*}{*/},%
  morestring=[b]",%
  morestring=[b]',%
  showstringspaces=false%
}[keywords,comments,strings]%

\lstset{language=Scala,%
  mathescape=true,%
  columns=[c]fixed,%
  basewidth={0.5em, 0.40em},%
  basicstyle=\tt,%
  commentstyle=\color{gray}\itshape\ttfamily,
  xleftmargin=0.0cm
}

% \lstset{tabsize=2,
% basicstyle=\ttfamily\fontsize{9pt}{1em}\selectfont,
% commentstyle=\color{gray}\itshape\ttfamily,
% numbers=left, numberstyle=\scriptsize\color{gray}\ttfamily, language=scala,moredelim=[il][\sffamily]{?},mathescape=false,showspaces=false,showstringspaces=false,xleftmargin=15pt,escapechar=@, morekeywords=[1]{let,fn,val},deletekeywords={for},classoffset=0,belowskip=\smallskipamount
% }

\theoremstyle{definition}
\newmdtheoremenv[hidealllines=true,topline=true,bottomline=true,skipabove=\baselineskip,skipbelow=\baselineskip]{defn}{Definition}[section]

\theoremstyle{definition}
\newmdtheoremenv[hidealllines=true,topline=true,bottomline=true,skipabove=\baselineskip,skipbelow=\baselineskip]{prop}{Property}[section]

% \theoremstyle{nonumberplain}
% \theoremstyle{definition}
% \newmdtheoremenv*[hidealllines=true,topline=true,bottomline=true,skipabove=\baselineskip,skipbelow=\baselineskip]{defnn}{Definition}[section]
\newtheorem*{defn*}{Definition}

% \newtheorem{defn}{Definition}[section]
% \newenvironment{defn}
  % {\begin{mdframed}[style=warning]\begin{mdef}}
  % {\end{mdef}\end{mdframed}}

% comments and notes
\newcommand{\comment}[1]{}
\newcommand{\note}[1]{{\bf $\clubsuit$ #1 $\spadesuit$}}
\newcommand{\ifreport}[1]{#1}
%\newcommand{\ifreport}[1]{}

\newcommand{\ie}{{\em i.e.,~}}
\newcommand{\eg}{{\em e.g.,~}}
\newcommand{\term}[1]{\mbox{\texttt{#1}}}
\newcommand{\itl}[1]{\mbox{\textit{#1}}}

% commas and semicolons
\newcommand{\comma}{,\,}
\newcommand{\commadots}{\comma \ldots \comma}
\newcommand{\semi}{;\mbox{;};}
\newcommand{\semidots}{\semi \ldots \semi}

% spacing
\newcommand{\gap}{\quad\quad}
\newcommand{\biggap}{\quad\quad\quad}
\newcommand{\nextline}{\\ \\}
\newcommand{\htabwidth}{0.5cm}
\newcommand{\tabwidth}{1cm}
\newcommand{\htab}{\hspace{\htabwidth}}
\newcommand{\tab}{\hspace{\tabwidth}}
\newcommand{\linesep}{\ \hrulefill \ \smallskip}

\newcommand{\sectionline}{%
  \nointerlineskip \vspace{\baselineskip}%
  \hspace{\fill}\rule{0.5\linewidth}{.7pt}\hspace{\fill}%
  \par\nointerlineskip \vspace{\baselineskip}
}

% figures
\newcommand{\figurebox}[1]
        {\fbox{\begin{minipage}{\textwidth} #1 \medskip\end{minipage}}}
\newcommand{\twofig}[3]
        {\begin{figure*}[t]#3\ \hrulefill\
        \caption{\label{#1}#2}\end{figure*}}
\newcommand{\boxfig}[3]
        {\begin{figure*}\figurebox{#3\caption{\label{#1}#2}}\end{figure*}}
\newcommand{\figref}[1]
        {Figure~\ref{#1}}

% arrays
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bda}{\[\ba}
\newcommand{\eda}{\ea\]}
\newcommand{\ei}{\end{array}}
\newcommand{\bcases}{\left\{\begin{array}{ll}}
\newcommand{\ecases}{\end{array}\right.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Language abstraction commands     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Relations
% Subtype
\newcommand{\sub}{<:}
% Type assignment
\newcommand{\typ}{:}
% reduction
\newcommand{\reduces}{\;\rightarrow\;}
% well-formedness
\newcommand{\wf}{\;\mbox{\textbf{wf}}}

%% Operators
% Type selection
\newcommand{\tsel}{\#}
% Function type
\newcommand{\tfun}{\rightarrow}
\newcommand{\dfun}[3]{(#1\!:\!#2) \Rightarrow #3}
% Conjunction
\newcommand{\tand}{\wedge}
% Disjunction
\newcommand{\tor}{\vee}
% Singleton type suffix
\newcommand{\sing}{.\textbf{type}}

%% Syntax
% Header for typing rules
\newcommand{\judgement}[2]{{\bf #1} \hfill \fbox{#2}}
% Refinement
\newcommand{\refine}[2]{\left\{#1 \Rightarrow #2 \right\}}
% Field definitions
\newcommand{\ldefs}[1]{\left\{#1\right\}}
% Member sequences
\newcommand{\seq}[1]{\overline{#1}}
% Lambda
\newcommand{\dabs}[3]{(#1\!:\!#2)\Rightarrow #3}
\newcommand{\abs}[3]{\lambda #1\!:\!#2.#3}
% Application
\newcommand{\app}[2]{#1\;#2}
% Substitution
\newcommand{\subst}[3]{[#1/#2]#3}
% Object creation
\newcommand{\new}[3]{\textbf{val }#1 = \textbf{new }#2 ;\; #3}
%\renewcommand{\new}[3]{#1 \leftarrow #2 \,\textbf{in}\, #3}
% Field declaration
\newcommand{\Ldecl}[3]{#1 \typ #2..#3}%{#1 \operatorname{>:} #2 \operatorname{<:} #3}
\newcommand{\ldecl}[2]{#1 \typ #2}
% Top and Bottom
\newcommand{\Top}{\top}%{\textbf{Top}}
\newcommand{\Bot}{\bot}%\textbf{Bot}}
% Environment extension
\newcommand{\envplus}[1]{\uplus \{ #1 \}}

\newcommand{\reduction}[4]{#1, #2 \reduces #3, #4}
\newcommand{\reducebig}[6]{#1, #2, #3 \;\Downarrow\; #4, #5, #6}
\newcommand{\evaluate}[5]{#1, #2, #3 \;\Downarrow\; #4, #5}
\newcommand{\evalcap}[8]{#1, #2, #3, #4 \;\Downarrow\; #5, #6, #7, #8}
\newcommand{\evalcapbreak}[8]{#1, #2, #3, #4 \;\Downarrow\; \\ #5, #6, #7, #8}
\newcommand{\evalcapfin}[9]{#2, #3, #4, #5 \;\Downarrow_{#1}\; #6, #7, #8, #9}

\newcommand{\sframe}[3]{\langle #1, #2, #3 \rangle}
\newcommand{\stack}[4]{#1 \sframe {#2} {#3} {#4}}
\newcommand{\reduce}[4]{#1, #2 \;\longrightarrow\; #3, #4}
\newcommand{\reducebreak}[4]{#1, #2 \\ \;\longrightarrow\; #3, #4}

\newcommand{\sreduce}[6]{#1, #2, #3 \;\longrightarrow\; #4, #5, #6}
\newcommand{\sreducebreak}[6]{#1, #2, #3 \\ \;\longrightarrow\; #4, #5, #6}
\newcommand{\sreducestar}[6]{#1, #2, #3 \;\longrightarrow^{\ast}\; #4, #5, #6}
\newcommand{\sreducestarbreak}[6]{#1, #2, #3 \\ \;\longrightarrow^{\ast}\; #4, #5, #6}

% misc identifiers
\newcommand{\dom}{\mbox{\sl dom}}
\newcommand{\fn}{\mbox{\sl fn}}
\newcommand{\bn}{\mbox{\sl bn}}
\newcommand{\sig}{\mbox{\sl sig}}
\newcommand{\IF}{\mbox{\mathem if}}
\newcommand{\OTHERWISE}{\mbox{\mathem otherwise}}
\newcommand{\strongexpand}{\prec\!\!\prec}
\newcommand{\weakexpand}{\prec}
\newcommand{\spcomma}{~,~}

% hide the copyright box
% \makeatletter
% \def\@copyrightspace{\relax}
% \makeatother


\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\setmainfont[Mapping=tex-text]{Times New Roman}
\setmonofont[Scale=0.8,BoldFont={Consolas Bold}]{Consolas}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country}
\copyrightyear{20yy}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish,
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers,
                                  % short abstracts)

% \titlebanner{banner above paper title}        % These are ignored unless
% \preprintfooter{short description of paper}   % 'preprint' option specified.

% \title{Function-Passing Style: A Functional Model of Concurrent Computation for Distributed Systems}
\title{Function-Passing Style: A Model for Typed, Distributed Functional Programming}
% \title{Function-Passing Style: Typed, Distributed Functional Programming}
% \subtitle{Subtitle Text, if any}

\authorinfo{Heather Miller\titlenote{Work done while the author was affiliated with Databricks.}}
           {EPFL}
           {heather.miller@epfl.ch}
\authorinfo{Philipp Haller}
           {KTH Royal Institute of Technology}
           {phaller@kth.se}

\maketitle

\begin{abstract}

The most successful systems for ``big data'' processing have all adopted
functional APIs. But the innards of these systems are often built atop
imperative and weakly-typed stacks, which complicates the design and
implementation of distributed system essentials like fault-tolerance. We
present a new programming model we call  {\em function-passing} (F-P),
designed to bring a bit of order to this disarray. F-P aims to provide a more
principled substrate on which to build data-centric distributed systems.

. F-P
seeks to

Typed, composable serialization. And an inversion of the actor model--send
functionality to data.

Can be viewed as a generalization of the MapReduce/Spark model.

More naturally models the .

A key approach is to pass safe, well-typed functions to immutable
distributed data, which in many cases improves latency (keep data distributed)
while making it easier by design to recover from failures. We provide an
operational semantics, an implementation in and for Scala, and a small
evaluation of the efficiency of our implementation of the model.

We describe this model in the context of the Scala programming language,
although there is no reason these concepts are limited to Scala.

Fault-handling built-in to the model. The user is free to choose their own
fault-handling strategy or lack thereof.

Evolve beautifully in time and space like a persistent data structure.

A model for asynchronous programming.

% The key idea is to provide
% --passing safe, well-typed functions to immutable distributed data.

% A model of concurrency which better suits communication in typed data-centric
% distributed programming. Our model is implemented in and for the Scala programming language--a language

%   we refer to as {\em function-passing}. model . We provide an operational
% semantics, an implementation in and for Scala, and a small evaluation of the
% efficiency of our implementation of the model.


\end{abstract}

\category{D.3.3}{Programming Languages}{Language Constructs and Features}
% general terms are not compulsory anymore,
% you may leave them out
\terms
Languages, Design

\keywords
Functional Programming, Distributed Programming, Functions, Closures,
Serialization, Concurrency, Types, Scala

\section{Introduction}

It's difficult to deny that data-centric programming is growing in importance.
At the same time, it's no secret that the most successful systems for
programming with ``big data'' have all adopted ideas from functional
programming; \ie programming with first-class functions and higher-order
functions. These functional ideas are often touted to be the key to the
success of these frameworks. It's not hard to imagine why--a functional,
declarative interface to data distributed over tens to thousands of nodes
provides a more natural way for end-users and data scientists to reason about
data.

While leveraging functional programming {\em concepts}, popular
implementations of the MapReduce~\cite{MapReduce} model, such as Hadoop
MapReduce~\cite{Hadoop} for Java, have been developed without making use of
functional language features such as closures. In contrast, a new generation
of programming systems for large-scale data processing, such as Apache
Spark~\cite{Spark}, Twitter's Scalding~\cite{Scalding}, and
Scoobi~\cite{Scoobi} build on functional language features in Scala in order
to provide high-level, declarative APIs.

% Spark~\cite{Spark}, MapReduce~\cite{MapReduce}, and Dryad~\cite{Dryad} are
% just a few.

% The benefits provided by functional
% programming have also won over framework designers as well--some have noticed
% that immutability, and data transformation via higher-order functions makes it
% much easier, by design, to tackle concerns central to distributed systems such
% as concurrency and fault tolerance.

%  -- the declarative, data-centric interface offered by such
% systems is a boon for mathematically-oriented data analytics.

However, these frameworks are built atop of tall stacks of typically
imperative and untyped code, losing most of the benefits enjoyed by the users
of their high-level APIs. Consequently, concerns central to distributed
systems such as concurrency and fault tolerance become more difficult to
reason about and realize in practice. Yet other problems manifest themselves
in all layers of the stack, surprising users and impacting ease-of-use,
complicating maintenance, and losing opportunities for optimization.
% Some such problems include:

\begin{itemize}

\item {\bf Complicated Ease of Use} These systems' APIs cannot statically
prevent {\em common usage errors}. As a result, users are often confronted
with runtime errors that are hard to debug. A common example is unsafe closure
serialization~\cite{Spores}.

\item {\bf Complicated Maintenance} Typically, only high-level user-facing abstractions are statically
typed.  The absence of static types in lower layers of the system makes
maintenance tasks, such as code refactorings, more difficult.

\item {\bf Lost Optimization Opportunities} The absence of certain kinds of
static type information precludes systems-centric optimizations. Importantly,
type-based static meta-programming enables fast serialization~\cite{Pickling},
but this is only possible if also lower layers (namely those dealing with
object serialization) are statically typed. Several
studies~\cite{JavaSerialization, JavaRMI, MoreEfficientJavaRMI, Jaguar} report
on the high overhead of serialization in widely-used runtime environments such
as the JVM. This overhead is so important in practice that popular systems,
like Spark~\cite{Spark} and Akka~\cite{Akka}, leverage alternative
serialization frameworks such as Protocol Buffers~\cite{Protobuf}, Apache
Avro~\cite{Avro}, or Kryo~\cite{Kryo}. \end{itemize}

Yet frameworks that took the leap to invert their models to  are riddled with
challenges at the level of the language. Scala is a language --
Spark~\cite{Spark}, Scalding~\cite{Scalding}, Kafka~\cite{Kafka}, and
Scoobi~\cite{Scoobi} are just a few.

% Yet, software developers still fumble with low-level RPC frameworks.

% In existing systems, types are only in the user-facing API and help the user.
% However, all too often, the internals of a distributed system are largely
% untyped, in particular when operating on data types that are also shipped
% remotely. In our approach we now go ahead and make types work so that they
% benefit both users (help- ing catch common errors) and distributed systems
% builders. Our approach leverages types to provide (1) type-specialized
% picklers, and (2) type-specialized collections/builders.

% Generalization of MapReduce model. Low-level. Inversion of the actor model.
% Can represent different many models for dis- tributed computing, e.g. Spark,
% Percolator (we probably can’t vali- date this claim.)

% What makes this problem even
% worse is the fact that coordinating between data shards and doing that in a
% way that is easy to reason about is the wheel that every data-centric
% distributed system keeps reinventing.

This paper takes a step towards more a more principled foundation for typed,
functional distributed programming. We present a new programming model, called
{\em function-passing}

Our work builds on two previous veins of work--type-safe and performant
serialization based on functional pickler combinators, and spores, closures
that are guaranteed to be serializable.

Guiding principles.

\begin{itemize}[noitemsep]
\item A solution should be rapidly uptakeable by real systems.
\end{itemize}

Our principal contribution is the careful design of a monadically inspired
model of concurrency. We provide a formal semantics, an implementation in
Scala, and a small empirical evaluation to show the benefits of some of the
proposed design decisions.

% Our principal contributions are a careful selection of features that
% facilitate important concerns like fault tolerance by design.

% that support
% the evolution of scripts into industrial grade programs—e.g., an expressive
% module system, an optional type annotation facility for declarations, and
% support for concurrency based on message passing between lightweight, isolated
% processes.


% Yet, building such systems remains an endeavor only succeeded by a few.

The actor model~\cite{Actors, ScalaActors}. While , well-typed operations on
remote data is not the model.  Closely related work here.

Structure. Our approach is to describe our model from a high level,
elaborating upon key concepts. While focusing on .. over time. We then proceed
to zoom in and make precise each component of our model in Sections~\ref{}.

% Functional programming concepts are making inroads in hot up-and-coming
% frameworks for distributed computing and, dare we say it, ``big data.''
% Arguably, MapReduce was one of the first systems to pick up FP concepts,
% albeit outside the context of functional languages. More recent frameworks are
% now also leveraging classical functional language features, such as higher-
% order functions (e.g., Spark). This is a boon for mathematically-oriented data
% analytics and higher-order functions provide flexible abstraction mechanisms.

% However, to provide fault tolerance in the context of large-scale distributed
% computing, such frameworks for distribution are built atop of tall stacks of
% code which is typically imperative and untyped, losing most of the benefits
% enjoyed by the users of their high-level APIs. [What's more,] the benefits of
% FP, abstraction, composition, equational reasoning, are seemingly lost by
% engineers building these distributed systems. What makes this problem even
% worse is the fact that coordinating between data shards and doing that in a
% way that is easy to reason about is the wheel that every data-centric
% distributed system keeps reinventing.

% Fortunately, FP techniques are not only useful for designing good user-facing
% APIs. This paper presents a deep connection between the pillars of FP and one
% of the most important challenges of distributed computing: fault tolerance. To
% this end we present a new programming model for typed distributed functional
% programming. This model aims to:

% \begin{itemize}
% \item distill existing fault recovery mechanisms based on lineage to their essence.
% Importantly, this paper shows that there is a direct correspondence between
% the concept of lineage, as it is widely used in distributed systems, and well-known
% pillars of FP. Our distributed programming model is a consequent
% implementation of this correspondence, and demonstrates it in executable form
% in the context of an implementation in and for Scala.

% \item improve type-safety at the boundary of serialized and deserialized data.
% Importantly, the design and implementation of our programming model shows that
% even the layer in a distributed system that processes freshly deserialized
% data (typically, this layer sits right on top of the network communication
% layer) can not only be written in a strongly-typed way, but it can even
% leverage performance improvements due to type-specialized serialization.
% \end{itemize}

% Our programming model is centered around a new distributed persistent data
% structure, the silo. A silo is a typed container for a single (immutable)
% value. A silo is ``stationary''; it is never moved away from the host on which
% it was created. Computations on silos are expressed using SiloRefs, proxy
% objects representing remote silos. The primary means to operate on a silo is
% by passing a serializable closure to it via a SiloRef. SiloRefs provide a
% standard monadic interface as well as primitives for defining flexible fault
% handling strategies.

% To ensure safe and efficient distribution of closures, our model leverages
% both syntactic and type-based restrictions. For instance, closures sent to
% remote silos are required to conform to the restrictions imposed by the so-called
% ``spore'' abstraction~\cite{Spores}. Among others, the syntax and
% static semantics of spores guarantees the absence of runtime serialization
% errors due to closure environments that are not serializable.

% The specific technical contributions of this paper are:

% \begin{itemize}
% \item A new programming model for functional processing of distributed data. By
% leveraging safe serializable closures, it prevents common usage errors of
% other widely-used data analytics stacks.

% \item A new model of lineage-based fault recovery based on typed FP. Our approach
% extends the capabilities of existing implementations by supporting type-specialized,
% statically-generated serializers even in the context of
% existentially-quantified types.

% \item A complete, distributed implementation of the model in Scala.

% \item A validation of spores in the context of distributed programming.

% \item Self-describing pickles?

% \item An experimental evaluation.
% \end{itemize}

\section{Overview of Model}

The best way to quickly visualize the F-P model is to think in terms of a
persistent functional data structure with structural sharing. Then, rather
than containing pure data, imagine instead that the structure represents a
directed acyclic graph (DAG) of transformations on distributed data.

Importantly, since this DAG of computations is a persistent data structure
itself, it is safe to exchange (copies of) subgraphs of a DAG between remote
nodes. This enables a robust and easy-to-reason-about model of fault
tolerance. We call subgraphs of a DAG lineages; lineages enable restoring the
data of failed nodes through re-applying the transformations represented by
their DAG. This sequence of applications must begin with data available from
stable storage.

Central to our model is the careful use of laziness. Computations on
distributed data are typically not executed eagerly; instead, applying a
function to distributed data just creates an immutable lineage. To obtain the
result of a computation, it is necessary to first ``kick off'' computation, or
to force its lineage. Within our programming model, this force
operation\todo{which force operation?} makes network communication (and thus
possibilities for latency) explicit, which is considered to be a strength when
designing distributed systems~\cite{ANoteDistComp}. Deferred evaluation also
enables optimizing distributed computations through operation fusion, which
avoids the creation of unnecessary intermediate data structures--this is
efficient in time as well as space. This kind of optimization is particularly
important and effective in distributed systems~\cite{FlumeJava}.\todo{What
does this have to do with FlumeJava?}

\vspace{-3mm}
\begin{center}\noindent\rule{8cm}{0.4pt}\end{center}
\begin{displayquote}
For these reasons, we believe that laziness should be viewed as an enabler in
the design of distributed systems.
\end{displayquote}
\vspace{-4mm}
\begin{center}\noindent\rule{8cm}{0.4pt}\end{center}
\vspace{1mm}

\noindent The F-P model consists of three main components:
\begin{itemize}[noitemsep]
  \item {\bf Silos:} stationary typed data containers.
  \item {\bf SiloRefs:} references to local or remote Silos.
  \item {\bf Spores:} safe, serializable functions.
\end{itemize}
\vspace{1mm}

\paragraph{Silos}
A silo is a typed data container. It is stationary in the sense that it does
not move between machines -- it remains on the machine where it was created.
Data stored in a silo is typically loaded from stable storage, such as a
distributed file system. A program operating on data stored in a silo can only
do so using a reference to the silo, a \verb|SiloRef|.

\paragraph{SiloRefs}
Similar to a proxy object, a SiloRef represents, and allows interacting with,
both local and remote silos. SiloRefs are immutable, storing identifiers to
locate possibly remote silos. SiloRefs are also typed (\verb|SiloRef[T]|)
corresponding to the type of their silo's data, leading to well-typed network
communication. The SiloRef provides three primitive operations/combinators
(some are lazy, some are not): map, flatMap, and send. map lazily applies a
user-defined function to data pointed to by the SiloRef, creating in a new
silo containing the result of this application. Like map, flatMap lazily
applies a user-defined function to data pointed to by the SiloRef. Unlike map,
the user-defined function passed to flatMap returns a SiloRef whose contents
is transferred to the new silo returned by flatMap. Essentially, flatMap
enables accessing the contents of (local or remote) silos from within remote
computations. We illustrate these primitives in more detail in Section~\ref{sec:primitives}.

\paragraph{Spores}

Spores~\cite{Spores} are safe closures that are guaranteed to be serializable
and thus distributable. They are a closure-like abstraction and type system
which gives authors of distributed frameworks a principled way of controlling
the environment which a closure (provided by client code) can capture. This is
achieved by (a) enforcing a specific syntactic shape which dictates how the
environment of a spore is declared, and (b) providing additional type-checking
to ensure that types being captured have certain properties.

\vspace{3mm}
\noindent A spore consists of two parts:

\begin{itemize}[noitemsep]
\item {\bf the spore header}, composed of a list of value definitions.
\item {\bf the spore body} (sometimes referred to as the “spore closure”), a regular closure.
\end{itemize}

\noindent This shape is illustrated below.

\vspace{-1mm}
\begin{figure}[h!]
\centering\includegraphics[width=0.75\columnwidth]{spore-shape.pdf}
\end{figure}
\vspace{-1mm}

The characteristic property of a spore is that the spore body is only allowed
to access its parameter, the values in the spore header, as well as top-level
singleton objects (Scala's form of modules). The spore closure is not allowed
to capture variables other than those declared in the spore header (\ie a
spore may not capture variables in the environment). By enforcing this shape,
the environment of a spore is always declared explicitly in the spore header,
which avoids accidentally capturing problematic references. Moreover,
importantly for object-oriented languages like Scala, it's no longer possible
to accidentally capture the \verb|this| reference.

Spores also come with additional type-checking. Type information corresponding
to captured variables are included in the type of a spore. This enables
authors of distributed frameworks to customize type-checking of spores to, for
example, {\em exclude} a certain type from being captured by user-provided
spores. Authors of distributed frameworks may kick on this type-checking by
simply including information about excluded types (or other type-based
properties) in the signature of a method. A concrete example would be to
ensure that the \verb|map| method on \verb|RDD|s in Spark to accept only
spores which do not capture \verb|SparkContext| (a non-serializable internal
framework class).

For a deeper understanding, see either Appendix~\ref{appendix:spores} which
covers the semantics of spores or the corresponding publication~\cite{Spores}.

\begin{figure}[t!]
\centering\includegraphics[width=0.8\columnwidth]{basic-diagram.pdf}
\caption{Basic model.}\label{fig:basic-diagram}
\end{figure}

\subsection{Basic Usage}

The only way to interact with distributed data stored in silos is through the
use of SiloRefs. A SiloRef can be thought of as an immutable handle to the
remote data contained within a corresponding silo. Users interact with this
distributed data by applying functions to SiloRefs, which are transmitted over
the wire and later applied to the data within the corresponding silo. As is
the case for persistent data structures, when a function is applied to a piece
of distributed data via a SiloRef, a new SiloRef representing the transformed
data is returned.

The simplest illustration of the model is shown in Figure~\ref{fig:basic-diagram}
(time flows vertically from top to bottom). Here, we start with a
\verb|SiloRef[T]| which points to a piece of remote data contained within a
\verb|Silo[T]|. When the function shown as $\lambda$ of type $T \Rightarrow S$
is applied to \verb|SiloRef[T]| and ``forced'' (sent over the wire), a new
SiloRef of type \verb|SiloRef[S]| is immediately returned. Note that
\verb|SiloRef[S]| contains a reference to its parent SiloRef,
\verb|SiloRef[T]| (This is how {\em lineages} are constructed). Meanwhile, the
function is asynchronously sent over the wire and is applied to
\verb|Silo[T]|, eventually producing a new \verb|Silo[S]| containing the data
transformed by function $\lambda$. This new \verb|SiloRef[S]| can be
used even before its corresponding silo is materialized (\ie before the data
in \verb|Silo[S]| is computed) – the F-P framework queues up operations
applied to \verb|SiloRef[S]| and applies them when \verb|Silo[S]| is fully
materialized.

Several vastly different sorts of complicated DAGs can be asynchronously built
up this way. Though first, to see how this is possible, we need to develop a
clearer idea primitive operations available on SiloRefs and their semantics.
We describe these in the following section.


\subsection{Primitives}
\label{sec:primitives}

There are four basic primitive operations on SiloRefs that together can be
used to build the higher-order operations common to popular data-centric
distributed systems (how to build some of these higher-order operations is
described in Section~\ref{sec:higher-order-operations}). These primitives
include:

\begin{itemize}[noitemsep,nolistsep]
\item \verb|map|
\item \verb|flatMap|
\item \verb|send|
\item \verb|cache|
\end{itemize}

\paragraph{map}
The \verb|map| method takes a spore that is to be applied to the data in the
silo of the receiver SiloRef. Rather than immediately sending the spore across
the network, and waiting for the operation to finish, the \verb|map| method is
\emph{lazy}. Without involving any network communication, it immediately
returns a SiloRef referring to a new, lazily-created silo. This new SiloRef
only contains lineage information, namely, a reference to the original
SiloRef, a reference to the argument spore, and the information that it is the
result of a \verb|map| invocation. (The \verb|send| method is used to force
the materialization of the result silo, see below.) For example, given a silo
with a list of \verb|Person| records, the following application of \verb|map|
defines a silo containing only the records of adults:

\begin{lstlisting}
val persons: SiloRef[List[Person]] = ...
val adults = persons.map(spore { ps => ps.filter(p => p.age >= 18) })
\end{lstlisting}

\paragraph{flatMap}
Like \verb|map|, the \verb|flatMap| method takes a spore that is to be applied
to the data in the silo of the receiver SiloRef. However, the crucial
difference is in the type of the spore argument whose result type is a SiloRef
in this case. Semantically, the new silo created by \verb|flatMap| is defined
to contain the data of the silo that the user-defined spore returns. The
\verb|flatMap| combinator adds expressiveness to our model that is essential
to express more interesting computation DAGs. For example, consider the
problem of combining the information contained in two different silos
(potentially located on different hosts). Suppose the information of a silo
containing \verb|Vehicle| records should be enriched with other details only
found in the \verb|adults| silo. In the following, \verb|flatMap| is used to
create a silo of \verb|(Person, Vehicle)| pairs where the names of person and
vehicle owner match:

\begin{lstlisting}
val vehicles: SiloRef[List[Vehicle]] = ...
// adults that own a vehicle
val owners = adults.flatMap(spore {
  val localVehicles = vehicles // spore header
  ps =>
    localVehicles.map(spore {
      val localps = ps // spore header
      vs =>
      localps.flatMap(p =>
        // list of (p, v) for a single person p
        vs.flatMap { v =>
          if (v.owner.name == p.name) List((p, v))
          else List()
        }
      )
    })
})
\end{lstlisting}

Note that the spore passed to \verb|flatMap| declares the capturing of the
\verb|vehicles| SiloRef in its so-called ``spore header''. The spore header
spans all variable definitions between the spore marker and the parameter list
of the spore's closure. The spore header defines the variables that the
spore's closure is allowed to access. Essentially, spores limit the free
variables of their closure's body to the closure's parameters and the
variables declared in the spore's header. Within the spore's closure, it is
necessary to read the data of the \verb|vehicles| silo in addition to the
\verb|persons| list. This requires calling \verb|map| on \verb|localVehicles|.
However, \verb|map| returns a SiloRef; thus, invoking \verb|map| on
\verb|adults| instead of \verb|flatMap| would be impossible, since there would
be no way to get the data out of the silo returned by
\verb|localVehicles.map(..)|. With the use of \verb|flatMap|, however, the
call to \verb|localVehicles.map(..)| creates the final result silo, whose data
is then also contained in the silo returned by \verb|flatMap|.

Clearly, \verb|map| can be expressed in terms of \verb|flatMap|:

\begin{lstlisting}
def map[S](s: Spore[T, S]): SiloRef[S] = {
  this.flatMap(spore {
    val localSpore = s
    x =>
      val res = localSpore(x)
      SiloRef.fromFun(currentHost)(spore {
        val localRes = res
        () => localRes
      })
}
\end{lstlisting}

This should come as no surprise, given that \verb|flatMap| is the monadic bind
operation on SiloRefs, and \verb|SiloRef.fromFun|\todo{fromFun is now introduced later. fix this} is the monadic return
operation. The reason why \verb|map| is provided as one of the main operations
of SiloRefs is that direct uses of \verb|map| enable an important optimization
based on operation fusion. This optimization is explained following the
discussion of the third main operation of SiloRefs, \verb|send|.

\paragraph{send}
The final operation of SiloRefs is \verb|send|. It forces the lazy computation
defined by the given SiloRef. Forcing is explicit in our model, because it
requires sending the lineage to the remote node on which the result silo
should be created. Given that network communication has a latency several
orders of magnitude greater than accessing a word in main memory, say, we felt
an explicit send operation a judicious choice. To enable materialization of
remote silos to proceed concurrently, the \verb|send| operation immediately
returns a future. This future is then asynchronously completed with the data
of the given silo. Since calling \verb|send| will materialize a silo and send
its data to the current node, \verb|send| should only be called on silos with
reasonably small data. For example, Figure ?? shows the effect of invoking
\verb|send| on the \verb|labels| SiloRef from above.

The \verb|send| operation can also be used to simply cache a silo in main
memory, for example, because the silo is known to be accessed subsequently:

\begin{lstlisting}
def cache(): Future[Boolean] = this.flatMap(spore {
  val localDoneSiloRef = DoneSiloRef
  res => localDoneSiloRef.send()
}
\end{lstlisting}

\paragraph{cache}
The \verb|cache| method can be provided for convenience. It invokes
\verb|send| not directly on the given SiloRef (which would transfer all data
of the silo to the current node); instead, it first uses \verb|flatMap| to
create a new silo that will be completed with the trivial value (e.g., a
Boolean constant) of the \verb|DoneSiloRef| singleton object. Essentially,
invoking \verb|send| on this trivial SiloRef causes the resulting future to be
completed as soon as \verb|this| SiloRef has been materialized in main memory.



\subsection{Creating Silos}
\label{sec:creating-silos}

Besides a type definition for SiloRef, our framework also provides a companion
singleton object (Scala's form of modules). The singleton object provides
factory methods for obtaining SiloRefs referring to silos populated with some
initial data:\footnote{The given code snippets are minimally simplified for
clarity. No essential details are omitted.}

\begin{lstlisting}
object SiloRef {
  def fromTextFile(host: Host)(file: File): SiloRef[List[String]] = ...
  def fromFun[T](host: Host)(s: Spore[Unit, T]): SiloRef[T] = ...
}
\end{lstlisting}

Each of the factory methods has a \verb|host| parameter that specifies the
target host (address/port) on which to create the silo. Note that the
\verb|fromFun| method takes a spore closure as an argument to make sure it can
be serialized and sent to \verb|host|. In each case, the returned SiloRef
contains its \verb|host| as well as an identifier that uniquely identifies its
silo on the \verb|host|. To obtain this host-unique identifier, each factory
method has to communicate with the target \verb|host|.

The type \verb|SiloRef[T]| has the following main operations:

\begin{lstlisting}
trait SiloRef[T] {
  def map[S](s: Spore[T, S]): SiloRef[S]
  def flatMap[S](s: Spore[T, SiloRef[S]]): SiloRef[S]
  def send(): Future[T]
}
\end{lstlisting}


Nothing beats a video when something evolves in space in time, so we've also produced a short video to illustrate the model\footnote{\url{https://vimeo.com/120415626}}


\subsection{Fault Handling}
\label{sec:fault-handling}

Besides the primitives discussed so far, the programming model includes
overloaded variants that enable the definition of flexible fault handling
semantics. The main idea is to specify fault handlers for \emph{subgraphs of
computation DAGs}. Our guiding principle is to make the definition of the
failure-free path through a computation DAG as simple as possible, while still
enabling the handling of faults at the fine-granular level of individual
SiloRefs.

\paragraph{Defining fault handlers} Fault handlers may be specified whenever
the lineage of a SiloRef is extended. For this purpose, the introduced
\verb|map| and \verb|flatMap| primitives are overloaded. For example,
consider our previous example, but extended with a fault handler:

\begin{lstlisting}
val persons: SiloRef[List[Person]] = ...
val vehicles: SiloRef[List[Vehicle]] = ...
// copy of `vehicles` on different host `h`
val vehicles2 = SiloRef.fromFun(h)(spore {
  val localVehicles = vehicles
  () => localVehicles
})

val adults =
  persons.map(spore { ps => ps.filter(p => p.age >= 18) })

// adults that own a vehicle
def computeOwners(v: SiloRef[List[Vehicle]]) =
  spore {
    val localVehicles = v
    (ps: List[Person]) => localVehicles.map(...)
  }

val owners: SiloRef[List[(Person, Vehicle)]] =
  adults.flatMap(computeOwners(vehicles),
                 computeOwners(vehicles2))
\end{lstlisting}

Importantly, in the \verb|flatMap| call on the last line, in addition to
\verb|computeOwners(vehicles)|, the regular spore argument of \verb|flatMap|,
\verb|computeOwners(vehicles2)| is passed as an additional argument. The
second argument registers a \emph{failure handler} for the subgraph of the
computation DAG starting at \verb|adults|. This means that if during the
execution of \verb|computeOwners(vehicles)| it is detected that the
\verb|vehicles| SiloRef has failed, it is checked whether the SiloRef that the
higher-order combinator was invoked on (in this case, \verb|adults|) has a
failure handler registered. In that case, the failure handler is used as an
alternative spore to compute the result of \verb|adults.flatMap(..)|. In this
example, we specified \verb|computeOwners(vehicles2)| as the failure handler;
thus, in case \verb|vehicles| has failed, the computation is retried using
\verb|vehicles2| instead.





\section{Higher-Order Operations}
\label{sec:higher-order-operations}

Operations on distributed collections\todo{somehow mention Spark here so it's
clear what a dist coll is.}~such as \verb|union|, \verb|groupByKey|, or
\verb|join|, involve multiple data sets, possibly located on different nodes.
In the following we explain how such operations can be expressed using the
introduced primitives.\todo{need a more compelling into for PL people}

\paragraph{union}
The union of two unordered collections stored in two different silos can be
expressed directly using the above \verb|flatMap| primitive.

\paragraph{join}
Suppose we are given two silos with the following types:

\begin{lstlisting}
val silo1: SiloRef[List[A]]
val silo2: SiloRef[List[B]]
\end{lstlisting}

\noindent as well as two hash functions computing hashes for elements of
type \verb|A| and \verb|B|, respectively:

\begin{lstlisting}
val hashA: A => K = ...
val hashB: B => K = ...
\end{lstlisting}

The goal is to compute the hash-join of \verb|silo1| and \verb|silo2|:

\begin{lstlisting}
val hashJoin: SiloRef[List[(K, (A, B))]] = ???
\end{lstlisting}

To be able to use \verb|flatMap|, the types of the two silos first have to be
made equal, through initial \verb|map| invocations:


\begin{lstlisting}
val silo12: SiloRef[List[(K, Option[A], Option[B])] =
      silo1.map { x => (hashA(x), Some(x), None) }
val silo22: SiloRef[List[(K, Option[A], Option[B])] =
      silo2.map { x => (hashB(x), None, Some(x)) }
\end{lstlisting}

Then, we can use \verb|flatMap| to create a new silo (at some destination
place), which contains the elements of both \verb|silo12| and \verb|silo22|:

\begin{lstlisting}
val combined = SiloRef.flatMap(destPlace, silo12, silo22,
                              (elem, emitter) => emitter.emit(elem),
                              listBuilderFactory[...])
\end{lstlisting}

The combined silo contains triples of type \verb|(K, Option[A], Option[B])|.
Using an additional \verb|map|, the collection can be sorted by key, and adjacent
triples be combined, yielding a \texttt{SiloRef[List[(K, (A, B)} as required.


\paragraph{Partitioning and groupByKey}

A \verb|groupByKey| operation on a group of silos containing collections needs
to create multiple result silos, on each node, with ranges of keys supposed to
be shipped to destination nodes. These destination nodes are determined using
a partitioning function. Our goal, concretely:

\begin{lstlisting}
val groupedSilos = groupByKey(silos)
\end{lstlisting}

Furthermore, we assume that \verb|silos.size| $= N$ where $N$ is the number of
nodes, with nodes $N_1$, $N_2$, etc. We assume each silo contains an unordered
collection of key-value pairs (a multi-map). Then, \verb|groupByKey| can be
implemented as follows:

\begin{itemize}
\item For each node $N_i$, the master node creates $N$ SiloRefs.

\item Each node $N_i$ applies a {\em partitioning function}
(example: \texttt{hash(key) mod N}) to the key-value pairs in its silo,
yielding $N$ (local) silos.

\item Using \verb|flatMap|, each pair of silos containing keys of the
same range can be combined and materialized on the right destination node.
\end{itemize}


\section{Formalization}
\label{sec:formalization}

\begin{figure}[ht!]
  \centering

  $\ba[t]{l@{\hspace{2mm}}l}
t ::=     x                                 & \mbox{variable}
\\
\gap ~|~  (x: T) \Rightarrow t              & \mbox{abstraction}
\\
\gap ~|~  t~t                               & \mbox{application}
\\
\gap ~|~  \texttt{let}~x = t~\texttt{in}~t  & \mbox{let binding}
\\
\gap ~|~  \{ \seq{l = t} \}                 & \mbox{record construction}
\\
\gap ~|~  t.l                               & \mbox{selection}
\\
\gap ~|~  \texttt{spore}~\{~\seq{x : T = t}~; (x: T) \Rightarrow t~\}  & \mbox{spore}
\\
\gap ~|~  \texttt{map}(r, t)                & \mbox{map}
\\
\gap ~|~  \texttt{flatMap}(r, t[, t])       & \mbox{flatMap}
\\
\gap ~|~  \texttt{send}(r)                  & \mbox{send}
\\
\gap ~|~  \texttt{await}(\iota)             & \mbox{await future}
\\
\gap ~|~  r                                 & \mbox{SiloRef}
\\
\gap ~|~  \iota                             & \mbox{future}
\\
 & \\
v ::=     (x: T) \Rightarrow t              & \mbox{abstraction}
\\
\gap ~|~  \{ \seq{l = v} \}                 & \mbox{record value}
\\
\gap ~|~  p                                 & \mbox{spore value}
\\
\gap ~|~  r                                 & \mbox{SiloRef}
\\
\gap ~|~  \iota                             & \mbox{future}
\\
 & \\
p ::=     \texttt{spore}~\{~\seq{x : T = v}~; (x: T) \Rightarrow t~\}  & \mbox{spore value}
\\
 & \\
T ::=     T \Rightarrow T                   & \mbox{function type} \\
\gap ~|~  \{ \seq{l : T} \}                 & \mbox{record type}   \\
\gap ~|~  \mathcal{S}                       & \mbox{}
\\
\mathcal{S} ::= T \Rightarrow T~\{~\texttt{type}~\mathcal{C} = \seq{T}~\}   & \mbox{spore type}
\\
\gap ~|~  T \Rightarrow T~\{~\texttt{type}~\mathcal{C}~\}   & \mbox{abstract spore type}
\\
\ea$

  \vspace{1mm}
  \caption{Core language syntax.}
  \label{fig:syntax}
  \vspace{1mm}
\end{figure}

We formalize our programming model in the context of a standard, typed lambda calculus with records. Figure~\ref{fig:syntax} shows the syntax of our core language. Terms are standard except for the \texttt{spore}, \texttt{map}, \texttt{flatMap}, \texttt{send}, and \texttt{await} terms. A \texttt{spore} term creates a new spore. It contains a list of variable definitions (the spore header) and the spore's closure. A term $\texttt{await}(\iota)$ blocks execution until the future $\iota$ has been completed asynchronously. The \texttt{map}, \texttt{flatMap}, and \texttt{send} primitives have been discussed earlier.


\subsection{Operational semantics}\label{sec:opsem}

\begin{figure}[ht!]
  \centering

  $\ba[t]{l@{\hspace{2mm}}l}
h \in Hosts &
\\
i \in \mathbb{N} &
\\
 & \\
\iota  ::=  (h, i)                               & \mbox{location}
\\
 & \\
r ::=     \text{Mat}(\iota) & \mbox{materialized}
\\
\gap ~|~  \text{Mapped}(\iota, h, r, p)          & \mbox{lineage with \texttt{map}}
\\
\gap ~|~  \text{FMapped}(\iota, h, r, p, opt_f)  & \mbox{lineage with \texttt{flatMap}}
\\
 & \\
E      ::=  \epsilon & \mbox{message queue}
\\
\gap ~|~    \text{Res}(\iota, v) \texttt{::} E     & \mbox{response}
\\
\gap ~|~    \text{Req}(h, r, \iota) \texttt{::} E  & \mbox{request}
\\
  \ea$

  \vspace{1mm}
  \caption{Elements of the operational model.}
  \label{fig:elems-opsem}
  \vspace{1mm}
\end{figure}

In the following we give a small-step operational semantics of the primitives of our language. The semantics is clearly stratified into a deterministic layer and a non-deterministic (concurrent) layer. Importantly, this means our programming model can benefit from existing reasoning techniques for sequential programs. Program transformations that are correct for sequential programs are also correct for distributed programs. Our programming model shares this property with some existing approaches such as~\cite{ConcurrentHaskell}.

\paragraph{Notation and conventions.} We write $S' = S + (\iota \mapsto v)$ to express the fact that $S'$ maps $\iota$ to $v$ and otherwise agrees with $S$. We write $S(\iota) = \text{Some}(v)$ to express the fact that $S$ maps $\iota$ to $v$. We write $S(\iota) = \text{None}$ if $S$ does not have a mapping for $\iota$. Reduction is defined using reduction contexts~\cite{TAPL}. We omit the definition of reduction contexts, since they are completely standard.

\paragraph{Configurations.} The reduction rules of the deterministic layer define transitions of \emph{host configurations} $(t, E, S)^h$ of host $h$ where $t$ is a term, $E$ is a message queue, and $S$ is a silo store. The reduction rules of the non-deterministic layer define transitions of sets $H$ of host configurations. The reduced host configurations are chosen non-deterministically in order to express concurrency between hosts.

\paragraph{Fault handling.} In the interest of clarity we present the reduction rules in two steps. In the first step we explain simplified rules without fault handling semantics (Sections~\ref{sec:det-layer} and~\ref{sec:nondet-layer}). In the second step we explain how these simplified rules have to be refined in order to support the fault handling principles of our model (Section~\ref{sec:faults-opsem}).


\subsubsection{Decentralized identification}

A important property of our programming model is the fact that silos are uniquely identified using \emph{decentralized identifiers}. A decentralized identifier $\iota$ has two components: (a) the identifier of the host $h$ that created $\iota$, and (b) a name $i$ created fresh on $h$ (e.g., an integer value): $\iota = (h, i)$. Decentralized identifiers are important, since they reconcile two conflicting properties central to our model. The first property is building computation DAGs locally, without remote communication. This is possible using decentralized identifiers, since each host can generate new identifiers independently of other remote hosts. The second property is allowing SiloRefs to be freely copied between remote hosts. This is possible, since decentralized identifiers uniquely identify silos without the need for subsequent updates of their information; decentralized identifiers are immutable. This latter property is essential to enable computation DAGs that are \emph{immutable upon construction}. In our programming model, computation DAGs are created using the standard monadic operations of SiloRefs. In particular, the \texttt{flatMap} operation (monadic bind) in general requires that its argument spore captures SiloRefs that are subsequently copied to a remote host. Hence it is essential that SiloRefs and the decentralized identifiers they contain be freely copyable between remote hosts.

\subsubsection{Deterministic layer}\label{sec:det-layer}

We first consider the reduction rules of the deterministic layer shown in Figure~\ref{fig:opsem-determ}. The reduction rules for \texttt{map} (\textsc{R-Map}) and \texttt{flatMap} (\textsc{R-FMap}) do not involve communication with other hosts. In each case, a new SiloRef $r'$ is created that is derived from SiloRef $r$. The execution of the actual operation (\texttt{map} or \texttt{flatMap}, respectively) is deferred, and an object representing this derivation is returned. In both cases, the new SiloRef $r'$ refers to a silo created on host $h'$ by applying the spore value $p$ to the value of silo $r$. The first component of the Mapped and FMapped objects, $(h, i)$, is a fresh \emph{location} created by host $h$ to uniquely identify the result silo.

Most reduction rules are enabled when the current redex is an \texttt{await} term. The reduction of a term $\texttt{await}(\iota)$ only continues when store $S$ maps location $\iota$ to value $v$. In all other cases, the current host removes the next message from its message queue $E$. As shown in Figure~\ref{fig:elems-opsem} there are two types of messages: requests (Req) and responses (Res). A response $\text{Res}(\iota, v)$ tells its receiver that the silo at location $\iota$ has value $v$. A request $\text{Req}(h, r, \iota)$ is sent on behalf of host $h$ to request the value of silo $r$ at location $\iota$. The reception of a response $\text{Res}(\iota, v)$ is handled by adding a mapping $(\iota \mapsto v)$ to the store (rule \textsc{R-Res}). The reception of a request $\text{Req}(h', r, \iota'')$ is handled locally if materialization of the requested silo $r$ is deferred and the parent silo $r'$ in $r$'s lineage has not been materialized either. In this case, the host sends a request to materialize $r'$ to itself.

\subsubsection{Nondeterministic layer}\label{sec:nondet-layer}

All reduction rules in the nondeterministic layer involve communication between two hosts.

Reducing a term $\texttt{send}(r)$ appends a request $\text{Req}(h, r, \iota)$ to the message queue of host $h'$ of the requested silo $r$. In this case, host $h$ creates a unique location $\iota = (h, i)$ to identify the silo subsequently. Rules \textsc{R-Req1}, \textsc{R-Req2}, and \textsc{R-Req3} define the handling of request messages that cannot be handled locally. If the request can be serviced immediately (\textsc{R-Req1}), a response with the value $v$ of the requested silo $r$ is appended to the message queue of the requesting host $h'$. Rules \textsc{R-Req2} and \textsc{R-Reg3} handle cases where the requested silo is not already available in materialized form.


\begin{figure*}[t!]
  \centering
% \vspace{-7mm}
\begin{mathpar}

\inferrule[\textsc{R-Map}]
{ host(r) = h' \quad i~\text{fresh} \\
  r' = \text{Mapped}((h, i), h', r, p)
}
{ (R[\texttt{map}(r, p)], E, S)^h \longrightarrow (R[r'], E, S)^h }

\inferrule[\textsc{R-FMap}]
{ host(r) = h' \quad i~\text{fresh} \\
  r' = \text{FMapped}((h, i), h', r, p, \text{None})
}
{ (R[\texttt{flatMap}(r, p)], E, S)^h \longrightarrow (R[r'], E, S)^h }

\inferrule[\textsc{R-Await}]
{ S(\iota) = \text{Some}(v)
}
{ (R[\texttt{await}(\iota)], E, S)^h \longrightarrow (R[v], E, S)^h }

\inferrule[\textsc{R-Res}]
{ E = \text{Res}(\iota, v) \texttt{::} E' \quad S' = S + (\iota \mapsto v)
}
{ (R[\texttt{await}(\iota_f)], E, S)^h \longrightarrow (R[\texttt{await}(\iota_f)], E', S')^h }

\inferrule[\textsc{R-ReqLocal}]
{ E = \text{Req}(h', r, \iota'') \texttt{::} E' \quad r = \text{Mapped}(\iota, h, r', p) \quad r' \neq \text{Mat}(\iota_s) \quad S(\iota) = \text{None} \\
  loc(r') = \iota' \quad S(\iota') = \text{None} \\
  E'' = \text{Req}(h, r', \iota') \texttt{::} E
}
{ (R[\texttt{await}(\iota_f)], E, S)^h \rightarrow (R[\texttt{await}(\iota_f)], E'', S)^h
}

\end{mathpar}
  % \vspace{-3mm}
  \caption{Deterministic reduction.}
  \label{fig:opsem-determ}
  % \vspace{-3mm}
\end{figure*}


\begin{figure*}[t!]
  \centering
% \vspace{-7mm}
\begin{mathpar}

\inferrule[\textsc{R-Send}]
{ host(r) = h' \quad h' \neq h \quad i~\text{fresh} \\
  \iota = (h, i) \quad m = \text{Req}(h, r, \iota)
}
{ \{ (R[\texttt{send}(r)], E, S)^h, (t, E', S')^{h'} \} \cup H \rightarrow \{ (R[\iota], E, S)^h, (t, E' \cdot m, S')^{h'} \} \cup H
}

\inferrule[\textsc{R-Req1}]
{ E = \text{Req}(h', r, \iota') \texttt{::} E' \quad r = \text{Mat}(\iota) \\
  S(\iota) = \text{Some}(v) \quad m = \text{Res}(\iota', v)
}
{ \{ (R[\texttt{await}(\iota_f)], E, S)^h, (t, E'', S')^{h'} \} \cup H \rightarrow \{ (R[\texttt{await}(\iota_f)], E', S)^h, (t, E'' \cdot m, S')^{h'} \} \cup H
}

\inferrule[\textsc{R-Req2}]
{ E = \text{Req}(h', r, \iota') \texttt{::} E' \quad r = \text{Mapped}(\iota, h, r', p) \quad r' = \text{Mat}(\iota_s) \quad S(\iota) = \text{None} \\
  S(\iota_s) = \text{Some}(v) \quad p(v) = v' \quad S' = S + (\iota \mapsto v') \quad m = \text{Res}(\iota', v')
}
{ \{ (R[\texttt{await}(\iota_f)], E, S)^h, (t, E'', S'')^{h'} \} \cup H \rightarrow \{ (R[\texttt{await}(\iota_f)], E', S')^h, (t, E'' \cdot m, S'')^{h'} \} \cup H
}

\inferrule[\textsc{R-Req3}]
{ E = \text{Req}(h'', r, \iota'') \texttt{::} E'' \quad r = \text{FMapped}(\iota, h, \text{Mat}(\iota_s), p, \text{None}) \quad S(\iota) = \text{None} \quad S(\iota_s) = \text{Some}(v) \\
  \quad p(v) = r' \quad loc(r') = \iota' \quad S(\iota') = \text{None} \quad host(r') = h' \quad m = \text{Req}(h, r', \iota') \quad E''' = \text{Req}(h'', r', \iota'') \texttt{::} E''
}
{ \{ (R[\texttt{await}(\iota_f)], E, S)^h, (t, E', S')^{h'} \} \cup H \rightarrow \{ (R[\texttt{await}(\iota_f)], E''', S)^h, (t, E' \cdot m, S')^{h'} \} \cup H
}

\inferrule[\textsc{R-Req4}]
{ E = \text{Req}(h'', r, \iota'') \texttt{::} E'' \quad r = \text{FMapped}(\iota, h, \text{Mat}(\iota_s), p, \text{None}) \quad S(\iota) = \text{None} \quad S(\iota_s) = \text{Some}(v) \\
  \quad p(v) = r' \quad loc(r') = \iota' \quad S(\iota') = \text{Some}(v') \quad S'' = S + (\iota \mapsto v') \quad m = \text{Res}(\iota'', v')
}
{ \{ (R[\texttt{await}(\iota_f)], E, S)^h, (t, E', S')^{h''} \} \cup H \rightarrow \{ (R[\texttt{await}(\iota_f)], E'', S'')^h, (t, E' \cdot m, S')^{h''} \} \cup H
}

\end{mathpar}
  % \vspace{-3mm}
  \caption{Nondeterministic reduction.}
  \label{fig:opsem-nondeterm}
  % \vspace{-3mm}
\end{figure*}

% Failures may be detected whenever a message is sent to a non-local host. Note that this does not mean that message sends must be synchronous. Instead, our implementation interprets a failure to establish a network connection with a remote host as a failure of the remote host and thus a failure of all silos that it hosts.

\subsection{Fault handling}\label{sec:faults-opsem}

The key principles of the fault handling mechanism are:
\begin{itemize}
\item Whenever a message is sent to a non-local host $h$, it is checked whether $h$ is alive; if it is not, any silos located on $h$ are declared to have failed.
\item Whenever the value of a silo $r$ cannot be obtained due to another failed silo, $r$ is declared to have failed.
\item Whenever the failure of a silo $r$ is detected, the nearest predecessor $r'$ in $r$'s lineage that is not located on the same host is determined. If $r'$ has a fault handler $f$ registered, the execution of $f$ is requested. Otherwise, $r'$ is declared to have failed.
\end{itemize}


% A host either (a) discovers itself that a remote silo has failed (e.g., because it could not establish a network connection to the remote host), or (b) is notified that a remote silo has failed by a healthy host involved in its materialization.

\begin{figure*}[t!]
  \centering
% \vspace{-7mm}
\begin{mathpar}

\inferrule[\textsc{RF-Send}]
{ host(r) = h' \quad h' \neq h \quad \text{failed}(h') \\
  \quad i~\text{fresh} \quad \iota = (h, i) \quad S'' = S + (\iota \mapsto \bot)
}
{ \{ (R[\texttt{send}(r)], E, S)^h \} \cup H \rightarrow \{ (R[\iota], E, S'')^h \} \cup H
}

\inferrule[\textsc{RF-Req4}]
{ E = \text{Req}(h'', r, \iota'') \texttt{::} E'' \quad r = \text{FMapped}(\iota, h, r'', p, \text{Some}(p_f)) \quad S(\iota) = \text{None} \\
  loc(r'') = \iota_s \quad S(\iota_s) = \text{Some}(v) \quad p(v) = r' \quad \text{failed}(host(r')) \quad p_f(v) = r_f \quad host(r_f) = h_f \quad \lnot \text{failed}(h_f) \\
  loc(r_f) = \iota_f \quad S(\iota_f) = \text{None} \quad m = \text{Req}(h, r_f, \iota_f) \quad E''' = \text{Req}(h'', r_f, \iota'') \texttt{::} E''
}
{ \{ (R[\texttt{await}(\iota_f)], E, S)^h, (t, E', S')^{h_f} \} \cup H \rightarrow \{ (R[\texttt{await}(\iota_f)], E''', S)^h, (t, E' \cdot m, S')^{h_f} \} \cup H
}

\inferrule[\textsc{RF-Req5}]
{ E = \text{Req}(h'', r, \iota'') \texttt{::} E' \quad r = \text{FMapped}(\iota, h, r'', p, \text{None}) \quad S(\iota) = \text{None} \quad loc(r'') = \iota_s \quad S(\iota_s) = \text{Some}(v) \\
  p(v) = r' \quad \text{failed}(host(r')) \quad i_p, i_a~\text{fresh} \quad \iota_p = (h, i_p), \iota_a = (h, i_a) \quad m_p = \text{ReqF}(h, r'', \iota_p) \\
  r_a = \text{FMapped}(\iota_a, h, \text{Mat}(\iota_p), p, \text{None}) \quad E'' = m_p \texttt{::} \text{Req}(h'', r_a, \iota'') \texttt{::} E'
}
{ (R[\texttt{await}(\iota_f)], E, S)^h \longrightarrow (R[\texttt{await}(\iota_f)], E'', S)^h
}

\inferrule[\textsc{RF-ReqF}]
{ E = \text{ReqF}(h, r, \iota') \texttt{::} E' \quad r = \text{FMapped}(\iota, h, r'', p, \text{Some}(p_f)) \\
  loc(r'') = \iota_s \quad S(\iota_s) = \text{Some}(v) \quad E'' = \text{Res}(\iota', p_f(v)) \texttt{::} E'
}
{ (R[\texttt{await}(\iota_f)], E, S)^h \longrightarrow (R[\texttt{await}(\iota_f)], E'', S)^h
}


\end{mathpar}
  % \vspace{-3mm}
  \caption{Fault handling.}
  \label{fig:opsem-faults}
  % \vspace{-3mm}
\end{figure*}

These principles are embodied in the reduction as follows. First, we use the
predicate $\text{failed}(h)$ as a way to check whether it is possible to
communicate with host $h$ (e.g., an implementation could check whether it is
possible to establish a socket connection). Second, failures of hosts are
handled whenever communication is attempted: whenever a host $h$ intends to
send a message to a host $h'$ where $h' \neq h$, it is checked whether
$\text{failed}(h')$. If it is the case that $\text{failed}(h')$, either the
corresponding location (silo or future) is declared as failed (and fault
handling deferred), or a suitable fault handler is located and a recovery step
is attempted. In the following we explain the extended reduction rules shown
in Figure~\ref{fig:opsem-faults}.

In rule \textsc{RF-Send}, the host of the requested silo $r$ is detected to have
failed. However, the parent silos of $r$ are all located on the same (failed)
host. Thus, in this case silo $r$ is simply declared as failed, and fault
handling is delegated to other parts of the computation DAG that require the
value of $r$ (if any). Since \texttt{send} is essentially a ``sink'' of a DAG,
no suitable fault handler can be located at this point.

This is different in rule \textsc{RF-Req4}. Here, host $h$ processes a message
requesting silo $r$ which is the result of a \texttt{flatMap} call.
Materializing $r$ requires obtaining the value of silo $r'$, the result of
applying spore $p$ to the value $v$ of the materialized parent $r''$.
Importantly, if the host of $r'$ is failed, it means the computation of the
DAG defined by spore $p$ did not result in a silo on an available host.
Consequently, if the \texttt{flatMap} call deriving $r$ specified a fault
handler $p_f$, $p_f$ is applied to $v$ in order to recover from the failure.
If the host of the resulting silo $r_f$ is not failed, the original request
for $r$ is ``modified'' to request $r_f$ instead. This is done by removing
message $\text{Req}(h'', r, \iota'')$ from the message queue and prepending
message $\text{Req}(h'', r_f, \iota'')$. Moreover, host $h$ sends a message to
itself, requesting the value of silo $r_f$.

Rule \textsc{RF-Req5} shows fault recovery in the case where the lineage of a
requested silo does not specify a fault handler itself. In this case, host $h$
creates two fresh locations $\iota_p, \iota_a$. $\iota_p$ is supposed to be
eventually mapped to the result value of executing the fault handler of parent
silo $r''$. Host $h$ requests this value from itself using a special message
$\text{ReqF}(h, r'', \iota_p)$. Finally, the original request for silo $r$ in
message queue $E$ is replaced with a request for silo $r_a$. The silo $r_a$ is
created analogous to $r$, but using silo $\text{Mat}(\iota_p)$ as parent
(eventually, location $\iota_p$ is mapped to the result of applying the
parent's fault handler). As demonstrated by rule \textsc{RF-ReqF}, ReqF
messages used to request the application of the fault handler are handled in a
way that is completely analogous to the way regular Req messages are handled,
except that fault handlers $p_f$ are applied as opposed to regular spores $p$.


\section{Implementation}
\label{sec:implementation}

\section{Evaluation}
\label{sec:evaluation}

\section{Related Work}
\label{sec:related-work}

Actors~\cite{Actors, ScalaActors}. Typed Actors~\cite{TypedActors}.

The Clojure programming language proposes Agents~\cite{Clojure}--stationary
mutable data containers that users apply functions to in order to update an
agent's state. F-P, in contrast, proposes that data in stationary containers
be immutable, and that transformations by function application form a
persistent data structure. Further, Clojure's Agents are designed to manage
state in a shared memory scenario, whereas F-P is designed with remote
references for a distributed scenario.

Cloud Haskell~\cite{CloudHaskell} leverages guaranteed-serializable, static
closures for a message-passing communication model inspired by Erlang. In
contrast, in our model spores are sent between passive, persistent silos.\todo{more here}
Closures and continuations in Termite Scheme~\cite{TermiteScheme} are always
serializable; references to non-serializable objects (like open files) are
automatically wrapped in processes that are serialized as their process ID.
Similar to Cloud Haskell, Termite is inspired by Erlang. In contrast to
Termite, F-P is statically typed, enabling advanced type-based optimizations.
In non-process-oriented models, parallel closures~\cite{ParallelClosures} and
RiverTrail~\cite{RiverTrail} address important safety issues.\todo{more here}

Alice ML~\cite{AliceML} is an extension of Standard ML which adds a number of
important features for distributed programming \todo{more here}. The design
leading up to F-P has incorporated many similar ideas, such as {type-safe},
generic and platform-independent pickling. F-P differs however, in that it is
a programming model rather than a language which focuses on sending functions
to immutable data rather than remote invocation.

Acute ML~\cite{AcuteML} propose.

Other approaches such as type-safe distributed OCaml~\cite{DistOCaml}

Other clean slate language designs have been proposed to broadly address
issues related to distributed programming. Thorn~\cite{Thorn} was designed
with concurrency and the need to interact with remote services in mind. Though
a main design goal of Thorn is to evolve scripts into typed programs via
gradual typing. F-P on the other hand is not a language, but a programming
model, which is designed to inject types into distributed program from the
get-go.

F-P integrates a distributed, persistent data structure. Other prior work
related to spores is discussed in~\cite{Spores}.

Type-safe distributed programming in ML5~\cite{Tom7} introduces a notion of a
typed context called a ``world'' and permits functions to be executed given a
specific world.

MapReduce~\cite{MapReduce}. Spark~\cite{Spark}. Dryad~\cite{Dryad}. Session
Types~\cite{SessionTypes}. AmbientTalk~\cite{AmbientTalk}. E
Language~\cite{ELang}.



\section{Conclusion and Future Work}
\label{sec:conclusion-future-work}

\section{Old}

\subsection{Old Intro}

% From the ivory tower, it may not be evident,
% It might not seem this way from the ivory tower,
% but

While it might not seem that way at a glance, mainstream software development
has become largely distributed. Two {\em styles} of distribution
dominate the current landscape; (1) systems composed of microservices,
 % or designed to be software as a service (SaaS),
and (2) systems for ``big data'' processing. Names like Netflix, SoundCloud,
and Twitter have established their competitive offerings by way of one or
both of these types of systems.

Microservices are small, independent (separately-compiled) services running on
different machines which communicate with each other to together make up a
single and complex application. ``Big data''-style applications on the other
hand are typically single applications whose data cannot fit into the memory
of one machine alone. Such applications are typically singly-compiled, with
their binaries distributed across a cluster of machines.

Yet, software developers still fumble with low-level RPC frameworks.
% and sluggish serialization frameworks.
Mainstream programming languages have traditionally offered little support in
this space.

We design module systems, etc, for maximum reuse and productivity, we go as
far as we can to demonstrate their power. But we often . Some of these
concerns have been affectionately referred to as the ``awkward
squad''~\cite{AwkwardSquad} by PL research, and they are concerns that are central to
contemporary software development.

Functional programming brings value to distributed systems builders. The
success of popular Spark can be attributed to functional ideas.

However, beyond Spark's interface, .

We refer to the model as {\em function-passing}, and .


We design a programming model in such a way that we can ``maximize'' static
types, and with it enable type-specialized picklers (statically generated),
typed closures, and type-specialized data structures.

The motto is ``making type inference work for you when optimizing distributed
systems'', or ``how type inference and existential types benefit distributed
systems''.

In existing systems, types are only in the user-facing API and help the user.
However, all too often, the internals of a distributed system are largely
untyped, in particular when operating on data types that are also shipped
remotely. In our approach we now go ahead and make types work so that they
benefit both users (helping catch common errors) \emph{and} distributed
systems builders. Our approach leverages types to provide (1) type-specialized
picklers, and (2) type-specialized collections/builders.

The approach is novel, because noone talks about making sure the unpicklers
and builders themselves are serializable, but that's actually fundamental to
the whole design.

\begin{itemize}

\item We have to make the point that serialization is like a primitive in
systems design, not some extra thing you figure out how to do later, like in
typical PLs. so maybe one central point that we should try to make is that
for distributed systems, serializability is like a key primitive, along with
latency and other things.

\item we want to prove that typed distributed systems are a good idea. right
now a lot of systems for dist computing are fundamentally untyped or
dynamically checked. those that are typed lose type information between
machines.

\item so we're trying to argue that types are good for system builders.

\end{itemize}

The fact that actually these benefits from types \emph{carry over to other
models as well} (not just spark-like models).


Generalization of MapReduce model. Low-level. Inversion of the actor model.
Can represent different many models for distributed computing, e.g. Spark,
Percolator (we probably can't validate this claim.)

Illustrations to have:
\begin{enumerate}
\item nice figure
\item organic evolving model on youtube
\end{enumerate}

\subsection{Evaluation}

we could make the performance eval strong by saying, ok, we implemented real
apps, so we have the real communication that needs to happen (and happens).
and we now make this communication typed, plus we measure exactly the
percentage of time that's spent in serialization for example. and then we
could even interpolate the results, so we could say, ok, if some other dist
system has a lower percentage spent in serialization, then the speed up would
be like this.

and then we could even measure that for real spark, and then interpolate and
say, ok, in the ideal case, meaning using all of our design principles, we
could speed up things like this, if one would re-architect spark to preserve
types (it's a huge task, so out of scope).

well, it would definitely be something that we can measure objectively. the
only thing we have to guard against would be to say, adding picklers and
builders does not otherwise \emph{degrade} performance, like, we'd have to
prove that there's no cost, but \emph{just} the performance benefits. adding
the classname of the unpickler adds a few bytes to each pickle, for example.

we could even show that with babyspark, like have a version that uses java
serialization, then a version that uses pickling without selfdescribing, then
with runtime unpickling, and with all static. because we have those numbers
for babyspark then the interpolation will be much more precise.



\section{Future Work}

Adapt this model for streaming computation. This would take coming up with a
solution to null out references in the {\em lineage}. This would make it possible to instantiate and populate new silos to handle incoming data.

\appendix
\section{Spores}
\label{appendix:spores}

% This is the text of the appendix, if you need one.

% \acks

% Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.
\bibliography{bib}

% \begin{thebibliography}{}
% \softraggedright

% \bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
% P. Q. Smith, and X. Y. Jones. ...reference text...

% \end{thebibliography}


\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices

