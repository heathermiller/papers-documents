It is difficult to deny that data-centric programming is growing in importance.
At the same time, it is no secret that the most successful systems for
programming with ``big data'' have all adopted ideas from functional
programming; \ie programming with first-class functions and higher-order
functions. These functional ideas are often touted to be the key to the success
of these frameworks. It is not hard to imagine why--a functional, declarative
interface to data distributed over tens to thousands of nodes provides a more
natural way for end-users and data scientists to reason about data.

While leveraging functional programming {\em concepts}, popular implementations
of the MapReduce~\cite{MapReduce} model, such as Hadoop MapReduce~\cite{Hadoop}
for Java, have been developed without making use of functional language
features such as closures. In contrast, a new generation of programming systems
for large-scale data processing, such as Apache Spark~\cite{Spark}, Twitter's
Scalding~\cite{Scalding}, and Scoobi~\cite{Scoobi} build on functional language
features in Scala in order to provide high-level, declarative APIs.

% Spark~\cite{Spark}, MapReduce~\cite{MapReduce}, and Dryad~\cite{Dryad} are
% just a few.

% The benefits provided by functional programming have also won over framework
% designers as well--some have noticed that immutability, and data
% transformation via higher-order functions makes it much easier, by design, to
% tackle concerns central to distributed systems such as concurrency and fault
% tolerance.

% -- the declarative, data-centric interface offered by such systems is a boon
% for mathematically-oriented data analytics.

However, these frameworks are built atop of tall stacks of typically imperative
and untyped code, losing most of the benefits enjoyed by the users of their
high-level APIs. Consequently, concerns central to distributed systems such as
concurrency and fault tolerance become more difficult to reason about and
realize in practice. Some language features, like closures, are not able to be
reliably distributed. Yet other problems manifest themselves in all layers of
the stack, surprising users and impacting ease-of-use, complicating
maintenance, and losing opportunities for optimization. Some such problems
include:

\begin{itemize}

\item {\bf Difficulty of Use} These systems' APIs cannot statically prevent
  {\em common usage errors} resulting from some language features not being
  designed with distribution in mind, often confronting users with
  hard-to-debug runtime errors. A common example is unsafe closure
  serialization~\cite{Spores}.

\item {\bf Complicated Maintenance} Typically, only high-level user-facing
  abstractions are statically typed.  The absence of static types in lower
  layers of the system makes maintenance tasks, such as code refactorings, more
  difficult.

\item {\bf Lost Optimization Opportunities} The absence of certain kinds of
  static type information precludes systems-centric optimizations. Importantly,
  type-based static meta-programming enables fast
  serialization~\cite{Pickling}, but this is only possible if also lower layers
  (namely those dealing with object serialization) are statically typed.
  Several studies~\cite{JavaSerialization, JavaRMI, MoreEfficientJavaRMI,
  Jaguar} report on the high overhead of serialization in widely-used runtime
  environments such as the JVM. This overhead is so important in practice that
  popular systems, like Spark~\cite{Spark} and Akka~\cite{Akka}, leverage
  alternative serialization frameworks such as Protocol
  Buffers~\cite{Protobuf}, Apache Avro~\cite{Avro}, or Kryo~\cite{Kryo}.

\end{itemize}

We present a new programming model we call {\em function passing} (F-P)
designed to overcome most of these issues by providing a more principled
substrate on which to build data-centric distributed systems. It builds upon
two previous veins of work--an approach for generating type-safe and performant
pickler combinators~\cite{Pickling}, and spores~\cite{Spores}, closures that
are guaranteed to be serializable.  Our model attempts to fit the paradigm of
data-centric programming more naturally by extending monadic programming to the
network. Our model can be thought of as somewhat of a dual to the actor
model;\footnote{There are many variations and interpretations of the actor
  model; in saying our model is somewhat of a dual, we simply mean to highlight
that programmers need not focus on programming with typically stationary
message handlers. Instead, our model focuses on a monadic interface for
programming with data (and sending functions instead).} rather than keeping
functionality stationary and sending data, in our model, we keep data
stationary and send functionality to the data. This results in well-typed
communication by design, a common pain point for builders of distributed
systems in Scala.

Our model brings together immutable, persistent data structures, monadic
higher-order functions, strong static typing, and lazy evaluation--pillars of
functional programming--to provide a more type-safe, and easy to reason about
foundation for data-centric distributed systems. Interestingly, we found that
laziness was an enabler in our model, without complicating the ability to
reason about programs. Without optimizations based on laziness, we found this
model would be impractically inefficient in memory and time.

One important contribution of our model is a precise specification of the
semantics of functional fault recovery. The fault-recovery mechanisms of
widespread systems such as Apache Spark, MapReduce~\cite{MapReduce} and
Dryad~\cite{Dryad} are based on the concept of a \emph{lineage}~\cite{Lineage1,
Lineage2}. Essentially, the lineage of a data set combines (a) an initial data
set available on stable storage and (b) a sequence of transformations applied
to initial and subsequent data sets. Maintaining such lineages enables fault
recovery through recomputation. Practical implementations of lineage-based
fault recovery suffer from complex code bases, typically eschewing strong
static typing. This paper presents a principled approach to lineage-based fault
recovery in a purely functional setting--to our knowledge a novelty in the PL
literature.

Our programming model is also designed to enable adoption in real-world
languages and systems. On the one hand, our model can be thought of as a
generalization of the MapReduce/Spark computation model, which has been shown
to be widely portable. On the other hand, the core primitives of the
programming model can be implemented in any language that enables closures to
be serialized.

% To ensure safe and efficient distribution of closures, our model leverages
% both syntactic and type- based restrictions. For instance, closures sent to
% remote nodes are required to conform to the restrictions imposed by the so-
% called “spore” abstraction that the authors presented in previous work [12].
% Among others, the syntax and static semantics of spores can guarantee the
% absence of runtime serialization errors due to closure environments that are
% not serializable.

This paper makes the following contributions:

\begin{itemize} %[noitemsep]

\item {\bf\em A new data-centric programming model for functional processing of
  distributed data} which makes important concerns like fault tolerance simple
  by design. The main computational principle is based on the idea of sending
  safe, guaranteed serializable functions to stationary data. Using standard
  monadic operations our model enables creating immutable DAGs of computations,
  supporting decentralized distributed computations. Lazy evaluation enables
  important optimizations while keeping programs simple to reason about.

\item {\bf\em A formalization of our programming model} based on a small-step
  operational  semantics. To our knowledge it is the first formal account of
  fault recovery based on lineage in a purely functional setting. Inspired by
  widespread systems like Spark, our formalization closely models real-world
  fault recovery mechanisms. The presented semantics is clearly stratified into
  a deterministic layer and a concurrent/distributed layer. Importantly,
  reasoning techniques for sequential programs are not invalidated by the
  distributed layer.

\item {\bf\em A distributed implementation of the programming model} in and for
  Scala. We present experiments that show some of the benefits of the proposed
  design, and we report on a validation of spores in the context of distributed
  programming.

% \item {\bf\em A small experimental evaluation} to show the benefits of some
  % of  the proposed design decisions, and a validation of spores in the
  % context of distributed programming.

% An experimental evaluation comparing the performance of our framework with
  % Java serialization and Kryo on a number of data types used in real-world,
  % large-scale distributed applications and frameworks

\end{itemize}

% The rest of our paper is structured as follows.

Our approach is to describe our model from a high-level, elaborating upon key
benefits and trade-offs, and then to zoom in and make each component part of
our model more precise. We describe the basic model this way in
Section~\ref{sec:basics}. We go on to show in
Section~\ref{sec:hoo} how essential higher-order operations
on distributed frameworks like Spark can be implemented in terms of the
primitives presented in Section~\ref{sec:basics}. We present a
formalization of our programming model in Section~\ref{sec:formal}, and
an overview of its prototypical implementation in
Section~\ref{sec:impl}.
% We show a small experimental evaluation of the efficiency of our model in
% Section~\ref{sec:evaluation},
Finally, we discuss related work in Section~\ref{sec:relwrk}, and
conclude in Section~\ref{sec:future}.

% In this paper, we present object-oriented pickler combinators and a framework
% for generating them at compile-time, called scala/pickling, designed to be
% the default serialization mechanism of the Scala programming language. The
% static generation of OO picklers enables significant performance
% improvements, outperforming Java and Kryo in most of our benchmarks. In
% addition to high performance and the need for little to no boilerplate, our
% framework is extensible: using the type class pattern, users can provide both
% (1) custom, easily interchangeable pickle formats and (2) custom picklers, to
% override the default behavior of the pickling framework. In benchmarks, we
% compare scala/pickling with other popular industrial frameworks, and present
% results on time, memory usage, and size when pickling/unpickling a number of
% data types used in real-world, large-scale distributed applications and
% frameworks.

% Yet frameworks that took the leap to invert their models to  are riddled with
% challenges at the level of the language. Scala is a language --
% Spark~\cite{Spark}, Scalding~\cite{Scalding}, Kafka~\cite{Kafka}, and
% Scoobi~\cite{Scoobi} are just a few.

% Yet, software developers still fumble with low-level RPC frameworks.

% In existing systems, types are only in the user-facing API and help the user.
% However, all too often, the internals of a distributed system are largely
% untyped, in particular when operating on data types that are also shipped
% remotely. In our approach we now go ahead and make types work so that they
% benefit both users (help- ing catch common errors) and distributed systems
% builders. Our approach leverages types to provide (1) type-specialized
% picklers, and (2) type-specialized collections/builders.

% Generalization of MapReduce model. Low-level. Inversion of the actor model.
% Can represent different many models for dis- tributed computing, e.g. Spark,
% Percolator (we probably can’t vali- date this claim.)

% What makes this problem even worse is the fact that coordinating between data
% shards and doing that in a way that is easy to reason about is the wheel that
% every data-centric distributed system keeps reinventing.

% This paper takes a step towards more a more principled foundation for typed,
% functional distributed programming. We present a new programming model,
% called {\em function-passing}

% Our work builds on two previous veins of work--type-safe and performant
% serialization based on functional pickler combinators, and spores, closures
% that are guaranteed to be serializable.

% Guiding principles.

% \begin{itemize}[noitemsep]
%
% \item A solution should be rapidly uptakeable by real systems.
%
% \end{itemize}

% Our principal contribution is the careful design of a monadically inspired
% model of concurrency. We provide a formal semantics, an implementation in
% Scala, and a small empirical evaluation to show the benefits of some of the
% proposed design decisions.

% Our principal contributions are a careful selection of features that
% facilitate important concerns like fault tolerance by design.

% that support the evolution of scripts into industrial grade programs—e.g., an
% expressive module system, an optional type annotation facility for
% declarations, and support for concurrency based on message passing between
% lightweight, isolated processes.

% Yet, building such systems remains an endeavor only succeeded by a few.

% The actor model~\cite{Actors, ScalaActors}. While , well-typed operations on
% remote data is not the model.  Closely related work here.

% Functional programming concepts are making inroads in hot up-and-coming
% frameworks for distributed computing and, dare we say it, ``big data.''
% Arguably, MapReduce was one of the first systems to pick up FP concepts,
% albeit outside the context of functional languages. More recent frameworks
% are now also leveraging classical functional language features, such as
% higher- order functions (e.g., Spark). This is a boon for
% mathematically-oriented data analytics and higher-order functions provide
% flexible abstraction mechanisms.

% However, to provide fault tolerance in the context of large-scale distributed
% computing, such frameworks for distribution are built atop of tall stacks of
% code which is typically imperative and untyped, losing most of the benefits
% enjoyed by the users of their high-level APIs. [What's more,] the benefits of
% FP, abstraction, composition, equational reasoning, are seemingly lost by
% engineers building these distributed systems. What makes this problem even
% worse is the fact that coordinating between data shards and doing that in a
% way that is easy to reason about is the wheel that every data-centric
% distributed system keeps reinventing.

% Fortunately, FP techniques are not only useful for designing good user-facing
% APIs. This paper presents a deep connection between the pillars of FP and one
% of the most important challenges of distributed computing: fault tolerance.
% To this end we present a new programming model for typed distributed
% functional programming. This model aims to:

% \begin{itemize}
%
% \item distill existing fault recovery mechanisms based on lineage to their
% essence. Importantly, this paper shows that there is a direct correspondence
% between the concept of lineage, as it is widely used in distributed systems,
% and well-known pillars of FP. Our distributed programming model is a
% consequent implementation of this correspondence, and demonstrates it in
% executable form in the context of an implementation in and for Scala.

% \item improve type-safety at the boundary of serialized and deserialized
% data. Importantly, the design and implementation of our programming model
% shows that even the layer in a distributed system that processes freshly
% deserialized data (typically, this layer sits right on top of the network
% communication layer) can not only be written in a strongly-typed way, but it
% can even leverage performance improvements due to type-specialized
% serialization.
%
% \end{itemize}

% Our programming model is centered around a new distributed persistent data
% structure, the silo. A silo is a typed container for a single (immutable)
% value. A silo is ``stationary''; it is never moved away from the host on
% which it was created. Computations on silos are expressed using SiloRefs,
% proxy objects representing remote silos. The primary means to operate on a
% silo is by passing a serializable closure to it via a SiloRef. SiloRefs
% provide a standard monadic interface as well as primitives for defining
% flexible fault handling strategies.

% To ensure safe and efficient distribution of closures, our model leverages
% both syntactic and type-based restrictions. For instance, closures sent to
% remote silos are required to conform to the restrictions imposed by the
% so-called ``spore'' abstraction~\cite{Spores}. Among others, the syntax and
% static semantics of spores guarantees the absence of runtime serialization
% errors due to closure environments that are not serializable.

% The specific technical contributions of this paper are:

% \begin{itemize}
%
% \item A new programming model for functional processing of distributed data.
% By leveraging safe serializable closures, it prevents common usage errors of
% other widely-used data analytics stacks.
%
% \item A new model of lineage-based fault recovery based on typed FP. Our
% approach extends the capabilities of existing implementations by supporting
% type-specialized, statically-generated serializers even in the context of
% existentially-quantified types.
%
% \item A complete, distributed implementation of the model in Scala.
%
% \item A validation of spores in the context of distributed programming.
%
% \item Self-describing pickles?
%
% \item An experimental evaluation.
%
% \end{itemize}
