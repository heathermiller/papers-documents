\documentclass[prodtf]{jfp1}
\usepackage{todonotes}

\usepackage{url}

%%% Terms
\newcommand{\FP}{{\emph{function passing}}}

%%% Abbreviations and emphasis
\newcommand{\ie}{{\em i.e.,~}}
\newcommand{\eg}{{\em e.g.,~}}

%%% Drafting
\newcommand{\comment}[1]{}
\newcommand{\fixme}[1]{{\color{gray}\itshape#1}}

\title[Journal of Functional Programming]{F-P \dots}

\author[H. Miller, P. Haller and N. M{\"u}ller]{%
  HEATHER MILLER\\ EPFL\\[1ex]%\email{heather.miller@epfl.ch}
  PHILIPP HALLER\\ KTH Royal Institute of Technology\\[1ex]%\email{phaller@kth.se}
  NORMEN M{\"U}LLER\\ Trivadis GmbH}%\\ \email{normen.mueller@trivadis.com}}

\jdate{April 2016}
\pubyear{2016}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\doi{...}

\bibliographystyle{jfp}

\begin{document}

\label{firstpage}

\maketitle

\begin{abstract}
  The most successful systems for ``big data'' processing have all adopted
  functional APIs. We present a new programming model we call {\FP} designed to
  provide a more principled substrate on which to build data-centric distributed
  systems. A key idea is to build up a persistent functional data structure
  representing transformations on distributed immutable data by passing
  well-typed serializable functions over the wire and applying them to this
  distributed data. Thus, the function passing model can be thought of as a
  persistent functional data structure that is {\emph{distributed}}, where
  transformations to data are stored in its nodes rather than the distributed
  data itself. The model simplifies failure recovery by design--data is
  recovered by replaying function applications atop immutable data loaded from
  stable storage. Deferred evaluation is also central to our model; by
  incorporating deferred evaluation into our design only at the point of
  initiating network communication, the function passing model remains easy to
  reason about while remaining efficient in time and memory. We formalize our
  programming model in the form of a small-step operational semantics which
  includes a semantics of functional fault recovery, and we provide an
  open-source implementation of our model in and for the Scala programming
  language, along with a case study of several example frameworks and end-user
  programs written atop of this model.
\end{abstract}

\tableofcontents

\section{Introduction}
It is difficult to deny that data-centric programming is growing in importance.
At the same time, it is no secret that the most successful systems for
programming with ``big data'' have all adopted ideas from functional
programming; \ie programming with first-class functions. These functional ideas
are often touted to be the key to the success of these frameworks. It is not
hard to imagine why--a functional, declarative interface to data, distributed
over tens to thousands of nodes, provides a more natural way for end-users and
data scientists to reason about data.

While leveraging functional programming {\emph{concepts}}, popular
implementations of the MapReduce~\cite{MapReduce} model, such as Hadoop
MapReduce~\cite{Hadoop} for Java, have been developed without making use of
functional language {\emph{features}} such as closures. For nearly a decade, the
open source interpretation of this model, Hadoop, swelled in size, remaining
largely unchallenged--causing nearly all of industry to synchronize on this one
implementation for most all large-scale data processing needs.

However, in recent years, a new generation of distributed systems for
large-scale data processing have suddenly cropped up, built on top of emerging
functional languages like Scala~\cite{???}; such systems include Apache
Spark~\cite{Spark}, Twitter's Scalding~\cite{Scalding}, and
Scoobi~\cite{Scoobi}. These systems make use of functional language features in
Scala in order to provide high-level, declarative APIs to end-users. Further,
the benefits provided by functional programming have also won over framework
designers--some have noticed that immutability, and data transformation
via higher-order functions makes it much easier, by design, to tackle concerns
central to distributed systems such as concurrency.

This sudden proliferation of new frameworks for distributed data-centric
programming, concurrent with the sudden growth in popularity of an emerging
programming language, begs the question--has it been our programming languages
that have limited us? Could it be that the primitives we build our systems upon
are too low-level, causing us to struggle to reinvent the same tricky wheel over
and over again? As these large-scale data processing applications continue to
grow in importance, what can we as language designers~\todo{"language
designers"?}\comment{sounds like we want to adapt/ modify/ improve one of those
previously mentioned "emerging programming languages"} do to make it easier for
more of these frameworks to rise?

This paper presents a new programming model\footnote{An abstraction of the
underlying computer system that allows for the expression of both algorithms and
data structures.} called the {\FP} model which has been designed to be a more
principled substrate (or middleware) upon which to build data-centric
distributed systems. It can be viewed as a generalization of the programming
models classified by MapReduce/Apache Spark--though it is not limited to, as we
will later show.

%% Intension:
%% In the subsequent two paragraphs, without going into details, I
%% want to clarify what we adopt and, on the other hand, to differentiate our
%% approach from those other programming models.

Traditional parallelism brings data to compute, whereas the aforementioned class
of programming models does the opposite--it brings compute to data. We adopt
this feature, known as {\emph{data locality}}, but elevate communication and
expressiveness via our unique combination of a framework for object-oriented
pickler combinators~\cite{Pickling} and a type-based foundation for
closures~\cite{Spores} with a novel concept of statically typed, yet
flexible\todo{Unlike RDD}\comment{Here I'm trying to hint the reader to the fact
that the concept of silos is more flexible than RDDs in terms of operations on
the "silo'd" data}, monadic silos\comment{How is our approach more flexible rsp.
more general than those other programming models? Pickling and Spores are one
thing, but how are silos more flexible than RDD[T]?}. These stationary,
immutable data structures can be remotely referenced. Transformations are
executed via the application of those closures, which are guaranteed to be
serializable. Performance and simplicity of persisting runtime objects are
ensured by the pickler framework.

This innovation enables two important benefits for distributed system builders:
(a) since all computations are functional transformations on type-flexible
monadic silos\footnote{In contrast to Apache Spark's RDDs~\cite{RDD}, those
statically typed, first order type constructors, give maximum freedom for user
defined operations on the siloed data.},\todo{RDD[T]'s map, flatMap give the
same degree of freedom?}\comment{If we do not differenciate our approach here,
item (a) is given by the adoptation, \ie does not belong to our innovation}
fault-tolerance is made simple by design, and (b) communication is made
well-typed by design, a common pain point for builders of distributed systems.

Said another way, the {\FP} model attempts to more naturally model the paradigm
of data-centric programming by extending monadic programming to the network.

\fixme{On this note, one might observe that the function passing model can
actually be interpreted as somewhat of a dual to the actor model;\footnote{There
are many variations and interpretations of the actor model; in saying our model
is somewhat of a dual, we simply mean to highlight that programmers need not
focus on programming with typically stationary message handlers. Instead, our
model focuses on a monadic interface for programming with data (and sending
functions instead).} rather than keeping functionality stationary and sending
data, in our model, we keep data stationary and send functionality to the data.}

\newpage
%\appendix
%\section{...}

\bibliography{bib}

\label{lastpage}

\end{document}
