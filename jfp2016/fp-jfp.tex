\documentclass{jfp1}
\usepackage{todonotes}

\usepackage{url}
\usepackage{amssymb}
\usepackage{bcprules}
\usepackage{mathpartir}
\usepackage{enumitem}
\usepackage{listings}

\usepackage{ntheorem}

\input{macros}

\title[Journal of Functional Programming]{F-P \dots\todo{Title}}

\author[H. Miller, P. Haller and N. M{\"u}ller]{%
  HEATHER MILLER\\ EPFL\\[1ex]%\email{heather.miller@epfl.ch}
  PHILIPP HALLER\\ KTH Royal Institute of Technology\\[1ex]%\email{phaller@kth.se}
  NORMEN M{\"U}LLER\\ Trivadis GmbH}%\\ \email{normen.mueller@trivadis.com}}

\jdate{April 2016}
\pubyear{2016}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\doi{...}

\bibliographystyle{jfp}

\begin{document}

\label{firstpage}

\maketitle

\begin{abstract}
  The most successful systems for ``big data'' processing have all adopted
  functional APIs. We present a new programming model we call {\FP} designed to
  provide a more principled substrate on which to build data-centric distributed
  systems. A key idea is to build up a persistent functional data structure
  representing transformations on distributed immutable data by passing
  well-typed serializable functions over the wire and applying them to this
  distributed data. Thus, the function passing model can be thought of as a
  persistent functional data structure that is {\em distributed}, where
  transformations to data are stored in its nodes rather than the distributed
  data itself. The model simplifies failure recovery by design--data is
  recovered by replaying function applications atop immutable data loaded from
  stable storage. Deferred evaluation is also central to our model; by
  incorporating deferred evaluation into our design only at the point of
  initiating network communication, the function passing model remains easy to
  reason about while remaining efficient in time and memory. We formalize our
  programming model in the form of a small-step operational semantics which
  includes a semantics of functional fault recovery, and we provide an
  open-source implementation of our model in and for the Scala programming
  language, along with a case study of several example frameworks and end-user
  programs written atop of this model.
\end{abstract}

\tableofcontents

\section{Introduction}

It is difficult to deny that data-centric programming is growing in importance.
At the same time, it is no secret that the most successful systems for
programming with ``big data'' have all adopted ideas from functional
programming; \ie programming with first-class functions. These functional
ideas are often touted to be the key to the success of these frameworks. It is
not hard to imagine why--a functional, declarative interface to data,
distributed over tens to thousands of nodes, provides a more natural way for
end-users and data scientists to reason about data.

While leveraging functional programming {\em concepts}, popular implementations
of Google's MapReduce~\cite{MapReduce} model, such as Apache Hadoop's MapReduce
Framework~\cite{Hadoop} for Java, have been developed without making use of
functional language {\em features} such as closures. For nearly a decade, the
Apache Hadoop open source interpretation of this model swelled in size,
remaining largely unchallenged--causing nearly all of industry to synchronize on
this one implementation for most all large-scale data processing needs.

However, in recent years, a new generation of distributed systems for
large-scale data processing have suddenly cropped up, built on top of emerging
functional languages like Scala~\cite{Odersky10}; such systems include Apache
Spark~\cite{Spark}, Twitter's Scalding~\cite{Scalding}, and
Scoobi~\cite{Scoobi}. These systems make use of functional language features in
Scala in order to provide high-level, declarative APIs to end-users. Further,
the benefits provided by functional programming have also won over framework
designers as well--some have noticed that immutability, and data transformation
via higher-order functions makes it much easier, by design, to tackle concerns
central to distributed systems such as concurrency.

%% What's the problem?
While widely adopted in practice, the aforementioned programming systems are not
without important issues. On the one hand, their programming interfaces do not
prevent common usage errors, such as unsafe closure serialization; as a result,
the complexities of distribution may trickle even to end users, who are
increasingly non-expert users. On the other hand, the foundations of their
programming models remain largely unclear, in particular, foundations of core
aspects such as fault tolerance, a critical aspect for distributed operation on
a large scale.

%% What the paper does
%%% Programming model
This paper introduces a new programming model which embraces the principle of
stationary data and mobile functions (``compute to data''). It can be viewed as
a generalization of the programming models classified by Google's
MapReduce/Apache Spark. Our programming model adopts the concept of {\em
lineage} which is used by the aforementioned systems to provide fault tolerance.
Lineage-based fault tolerance is facilitated by the core computational principle
of transformations on immutable, stationary data.

%%% Unique combination
The programming model is based on functional abstractions for lineage-based
distributed computation. In order to prevent common usage errors, the model
builds upon two previous veins of work--type-safe serialization based on
functional pickler combinators~\cite{Kennedy2004,Elsman2005,Pickling,AliceML}, and
serializable closures~\cite{CloudHaskell,Spores}. We believe this unique
combination of functional programming techniques provides a more principled
substrate upon which to build data-centric, distributed systems.

%%% Formalization
Moreover, we provide a complete formalization of the programming model in order
to study the foundations of lineage-based distributed computation. In
particular, we develop a theory of safe, mobile lineages based on a subject
reduction theorem for a typed core language. Thus, the formal model may serve as
a basis for further developments of the theory of data-centric distributed
programming, including aspects such as fault tolerance.

%% Contributions
\subsection{Contributions}

This paper makes the following contributions:
\begin{itemize}

  \item A new data-centric programming model for functional processing of
    distributed data which makes important concerns like fault tolerance simpler
    by design. The main computational principle is based on the idea of sending
    safe, guaranteed serializable functions to stationary data. Using standard
    monadic operations, our model enables creating immutable directed acyclic
    graphs (DAG) of computations, supporting decentralized distributed
    computations. Deferred evaluation enables important optimizations (operation
    fusion~\cite{FlumeJava}) while keeping programs simple to reason about.

  \item A formalization of lineage-based distributed computation based on a
    small-step operational semantics. Our formalization extends previous
    theories of serializable closures to {\em serializable lineages}. The
    technical development enabling this extension combines and generalizes (a)
    serializable types, (b) ``static'' closures, and (c) lineages.

  \item A proof of a subject reduction theorem for a typed, distributed core
    language based on lineages. To our knowledge we present the first such proof
    for a lineage-based distributed programming model.

  \item A proof establishing the preservation of lineage mobility by reduction
    for a typed, distributed core language. This property provides a foundation
    for lineage-based fault tolerance.

  \item A distributed implementation of the programming model in and for Scala
    as a middleware. In addition, we present prototype versions we have built of
    popular frameworks like Apache Spark and MBrace~\cite{MBrace} using the
    function passing model, and end-user applications we have built using each
    of these prototype frameworks.
  
\end{itemize}

Our approach is to describe our model from a high-level, elaborating upon key
benefits and trade-offs, and then to zoom in and make each component part of our
model more precise. We describe the basic model this way in
Section~\ref{sec:basic-model}. We go on to show in
Section~\ref{sec:higher-order-operations} how essential higher-order operations
on distributed frameworks like Apache Spark can be implemented in terms of the
primitives presented in Section~\ref{sec:basic-model}. We present a
formalization of our programming model in Section~\ref{sec:formalization}, and a
proof of a subject reduction theorem in Section~\ref{sec:subject-reduction}.
Section~\ref{sec:implementation} presents an overview of its prototypical
implementation. In Section~\ref{sec:examples}, we show examples of different
sorts of distributed frameworks built atop the function passing model. Finally,
we discuss related work in Section~\ref{sec:related-work}, and conclude in
Section~\ref{sec:conclusion-future-work}.

\section{Overview of Model}
\label{sec:basic-model}

\subsection{Essence}

In the broadest sense, the function passing model can be thought of as a sort of
persistent functional data structure with monadic operations and structural
sharing. However, rather than containing pure data, instead the data structure
represents a DAG of functional transformations on distributed data. The root
node contains immutable data read from stable storage (\eg Amazon S3); edges
represent functional transformations\comment{on immutable data represented as
nodes of the DAG.}\comment{\fixme{ yielding child nodes as placeholders, \ie,
immutable data resulting from those to be executed transformations.}}.

Importantly, since this DAG of computations is a persistent data structure
itself, it is safe to exchange (copies of) subgraphs of a DAG between remote
nodes. This enables a robust and easy-to-reason-about model of fault tolerance.
Subgraphs of the DAG are called \textit{lineages}; lineages enable restoring the 
data of failed nodes through re-applying their transformations. This sequence of 
applications must begin with data available from stable storage.

Central to the function passing model is the careful use of deferred evaluation.
Computations on distributed data are typically not executed eagerly; instead,
applying a function to distributed data just creates an immutable, local
lineage. To make a network call and thus obtain the result of a computation, it
is necessary to first ``kick off'' the computation in order to materialize the
nodes of its lineage. Within our programming model, this force operation (called
\verb|send()|) makes network communication (and thus possibilities for latency)
explicit, which is considered to be a strength when designing distributed
systems~\cite{ANoteDistComp}. Deferred evaluation also enables optimizing
distributed computations through operation fusion, which avoids the creation of
unnecessary intermediate data structures--this is efficient in time as well as
space. This kind of optimization is particularly important and effective in
distributed systems~\cite{FlumeJava}. For these reasons, we believe that
deferred evaluation should be viewed as an enabler in the design of distributed
systems.

\subsection{The Model}

The function passing model consists of three main components: \textbf{Silos}
stationary, typed, immutable data containers, \textbf{Silo references} to local
or remote Silos, and \textbf{Spores} safe, serializable functions.

\paragraph{Silos}

A silo is a typed and immutable data container. It is stationary in the sense
that it does not move between machines -- it remains on the machine where it was
created. Data stored in a silo is typically loaded from stable storage, such as
a distributed file system. A program operating on data stored in a silo can only
do so using a reference to the silo.

\paragraph{Silo references}

Similar to a proxy object, a silo reference represents, and allows interacting
with both local and remote silos. Silo references are immutable, storing
identifiers to locate possibly remote silos. They are also typed
(\verb|SiloRef[T]|) corresponding to the type \verb|T| of their silo's data,
leading to well-typed network communication. A silo reference provides three
primitive operations/combinators (some are lazy, some are not): \verb|map|,
\verb|flatMap|, and \verb|send|. The \verb|map| method makes use of deferred
evaluation; it {\em eventually} applies a user-defined function to data pointed
to by the \verb|SiloRef[T]|, creating a new silo containing the result of this
application, though this application. That is, this computation is only ``kicked
off'' when the \verb|send| method is invoked. This makes it possible to queue up
transformations in order to optimize network communication. Like \verb|map|, the
application of \verb|flatMap| is {\em deferred}. \verb|flatMap| also applies a
user-defined function to data pointed to by the silo reference; unlike
\verb|map|, however, the user-defined function passed to \verb|flatMap| returns
a \verb|SiloRef[T]| whose contents is transferred to the new silo returned by
\verb|flatMap|. Essentially, \verb|flatMap| enables accessing the contents of
(local or remote) silos from within remote computations. We illustrate these
primitives in more detail in Section~\ref{sec:primitives}.

\paragraph{Spores}

Spores~\cite{Spores} are safe closures that are guaranteed to be serializable
and thus distributable. They are a closure-like abstraction and a type system
which gives authors of distributed frameworks a principled way of controlling
the environment which a closure (provided by client code) can capture. This is
achieved by (a) enforcing a specific syntactic shape which dictates how the
environment of a spore is declared, and (b) providing additional type-checking
to ensure that types being captured have certain properties.

\vspace{3mm}
\noindent A spore consists of two parts: the \textbf{spore header}, composed of
a list of value definitions, and the \textbf{spore body} a regular closure;
sometimes referred to as the “spore closure”. This shape is illustrated in
Figure~\ref{fig:spore-shape}.

\begin{figure}[ht!]
\centering\includegraphics[width=0.75\columnwidth]{pic/spore-shape.pdf}
\caption{The shape of a spore.}\label{fig:spore-shape}
\end{figure}

\vspace{3mm}
The characteristic property of a spore is that the spore body is only allowed to
access its parameter, the values in the spore header, as well as top-level
singleton objects (Scala's form of modules). The spore closure is not allowed to
capture variables other than those declared in the spore header (\ie a spore may
not capture variables in the environment). By enforcing this shape, the
environment of a spore is always declared explicitly in the spore header, which
avoids accidentally capturing problematic references. Moreover, importantly for
object-oriented languages like Scala, it's no longer possible to accidentally
capture the \verb|this| reference.

Spores also come with additional type-checking. Type information corresponding
to captured variables are included in the type of a spore. This enables authors
of distributed frameworks to customize type-checking of spores to, for example,
{\em exclude} a certain type from being captured by user-provided spores.
Authors of distributed frameworks may kick on this type-checking by simply
including information about excluded types (or other type-based properties) in
the signature of a method. A concrete example would be to ensure that the
\verb|map| method on \verb|RDD|s in Apache Spark (a distributed collection)
accepts only spores which do not capture \verb|SparkContext| (a non-serializable
internal framework class).

\comment{For a deeper understanding of spores, see the corresponding
publication~\cite{Spores}.}

\begin{figure}[t!]
\centering\includegraphics[width=0.8\columnwidth]{pic/basic-diagram.pdf}
\caption{Basic function passing model.}\label{fig:basic-diagram}
\end{figure}

\subsection{Basic Usage}

We begin with a simple visual example to illustrate the basics of the function
passing model. % \footnote{We also feel that, to visually illustrate a system
%   evolving in both space and time, nothing beats a video animation. So, we've
%   produced a short animation to provide visual intuition of the function
%   passing model: \url{https://vimeo.com/120415626}}

The main handle users have to the framework is via \verb|SiloRef[T]|s. A
\verb|SiloRef[T]| can be thought of as an immutable handle to distributed data
\verb|T| contained within a corresponding silo. Users interact with this
distributed data by applying functions (as spores) to silo references. Those
functions are transmitted over the wire and later applied to the data within the
corresponding silo. As is the case for persistent data structures, when a
function is applied to a piece of distributed data via a \verb|SiloRef[T]|, a
new \verb|SiloRef[T']|, representing a new silo containing the transformed data
\verb|T'|, is returned.

The simplest illustration of the model is shown in
Figure~\ref{fig:basic-diagram} (time flows vertically from top to bottom). Here,
we start with a \verb|SiloRef[T]| which points to a piece of remote data
contained within a \verb|Silo[T]|. When the function, shown as $\lambda$, of
type $T \Rightarrow S$ is applied to \verb|SiloRef[T]| and ``forced'' (sent over
the wire), a new silo reference of type \verb|SiloRef[S]| is immediately
returned. Note that \verb|SiloRef[S]| contains a reference to its parent silo
reference, \verb|SiloRef[T]| (This is how {\em lineages} are constructed.).
Meanwhile, the function is asynchronously sent over the wire and is then applied
to \verb|Silo[T]|, eventually producing a new \verb|Silo[S]| containing the data
transformed by function $\lambda$. This new \verb|SiloRef[S]| can be used even
before its corresponding silo is materialized (\ie before the data in
\verb|Silo[S]| is computed) – the function passing framework queues up
operations applied to \verb|SiloRef[S]| and applies them when \verb|Silo[S]| is
fully materialized.

Different sorts of complex DAGs can be asynchronously built up in this way.
Though first, to see how this is possible, we need to develop a clearer idea of
the primitive operations available on silo references and their semantics. We
describe these in the following section.

\begin{figure}[ht!]
\centering\includegraphics[scale=0.35]{pic/bigger-dag.pdf}
\caption{A simple DAG in the function passing model.}\label{fig:bigger-dag}
\end{figure}

\subsection{Primitives}
\label{sec:primitives}

There are four basic primitive operations on silo references that together can
be used to build the higher-order operations common to popular data-centric
distributed systems (how to build some of these higher-order operations is
described in Section~\ref{sec:higher-order-operations}). In this section we'll
introduce these primitives in the context of a running example. These primitives
include: \verb|map|, \verb|flatMap|, \verb|send|, and \verb|cache|.

\paragraph{map}%
%
\texttt{def map[S](p: Spore[T, S]): SiloRef[S]} \newline
%
The \verb|map| method takes a spore that is to be applied to the data in the
silo associated with the given \verb|SiloRef|. Rather than immediately sending
the spore across the network, and waiting for the operation to finish, the
\verb|map| method's evaluation is {\em deferred}. Without involving any network
communication, it immediately returns a \verb|SiloRef| referring to a new,
to-be-created silo. This new silo reference only contains lineage information,
namely, a reference to the original \verb|SiloRef|, a reference to the argument
spore, and the information that it is the result of a \verb|map| invocation. As
we explain below, another method, \verb|send| or \verb|cache|, must be called
explicitly to force the materialization of the result silo.

To better understand how DAGs are created and how remote silos are materialized,
we will develop a running example throughout this section. Given a silo
containing a list of \verb|Person| records, the following application of
\verb|map| defines a (not-yet-materialized) silo containing only the records of
adults (graphically shown in Figure~\ref{fig:bigger-dag}, part 1):

\begin{lstlisting}
val persons: SiloRef[List[Person]] = ...
val adults: SiloRef[List[Person]] = persons.map(spore { ps =>
  ps.filter(p => p.age >= 18) })
\end{lstlisting}


\paragraph{flatMap}%
%
\texttt{def flatMap[S](p: Spore[T, SiloRef[S]]): SiloRef[S]} \newline
%
Like \verb|map|, the \verb|flatMap| method takes a spore that is to be applied
to the data in the silo of the given \verb|SiloRef[T]|. However, the crucial
difference is in the type of the spore argument whose result type is a
\verb|SiloRef[S]| in this case. Semantically, the new silo created by
\verb|flatMap| is defined to contain the data of the silo that the user-defined
spore returns. The \verb|flatMap| combinator adds expressiveness to our model
that is essential to express more interesting computation DAGs. For example,
consider the problem of combining the information contained in two different
silos (potentially located on different hosts). Suppose the information of a
silo containing \verb|Vehicle| records should be enriched with other details
only found in the \verb|adults| silo. In the following, \verb|flatMap| is used
to create a silo of \verb|(Person, Vehicle)| pairs where the names of person and
vehicle owner match (graphically shown in Figure~\ref{fig:bigger-dag}, part 2):

\begin{lstlisting}
val vehicles: SiloRef[List[Vehicle]] = ...
// adults that own a vehicle
val owners: SiloRef[List[(Person,Vehicle)]] = 
  adults.flatMap(spore {
    // spore header
    val localVehicles = vehicles 
    persons => localVehicles.map(spore {
      // spore header
      val localPersons = persons 
      vehicles => localPersons.flatMap(p =>
        // list of (p, v) for a single person p
        vehicles.flatMap { v => 
          if (v.owner.name == p.name) List((p, v))
          else Nil
        }
      )
    })
  })
\end{lstlisting}
\noindent
Note that the spore passed to \verb|flatMap| declares the capturing of the
\verb|vehicles| silo reference in its spore header. The spore header spans all
variable definitions between the spore marker and the parameter list of the
spore's closure. The spore header defines the variables that the spore's closure
is allowed to access. Essentially, spores limit the free variables of their
closure's body to the closure's parameters and the variables declared in the
spore's header. Within the spore's closure, it is necessary to read the data of
the \verb|vehicles| silo in addition to the \verb|persons|, the list of
\verb|Person| records. This requires calling \verb|map| on \verb|localVehicles|.
However, \verb|map| returns a silo reference; thus, invoking \verb|map| on
\verb|adults| instead of \verb|flatMap| would be impossible, since there would
be no way to get the data out of the silo returned by
\verb|localVehicles.map(..)|. With the use of \verb|flatMap|, however, the call
to \verb|localVehicles.map(..)| creates the final result silo, whose data is
then also contained in the silo returned by \verb|flatMap|.

Although the expressiveness of the \verb|flatMap| combinator subsumes that of
the \verb|map| combinator (see Section~\ref{sec:expr}), keeping \verb|map| as a
(lightweight) primitive enables more opportunities for optimizing computation
DAGs (\eg operation fusion~\cite{FlumeJava}).

\paragraph{send}%
%
\texttt{def send(): Future[T]} \newline
%
As mentioned earlier, the execution of computations built using silo references
is deferred. The \verb|send| operation {\em forces} the deferred computation
defined by the given \verb|SiloRef|. Forcing is explicit in our model, because it
requires sending the lineage to the remote node on which the result silo should
be created. Given that network communication has a latency several orders of
magnitude greater than accessing a word in main memory, providing an explicit
send operation is a judicious choice~\cite{ANoteDistComp}.

To enable materialization of remote silos to proceed concurrently, the
\verb|send| operation immediately returns a future~\cite{Futures}. This future
is then asynchronously completed with the data of the given silo. Since calling
\verb|send| will materialize a silo and send its data to the current node,
\verb|send| should only be called on silos with reasonably small data (for
example, in the implementation of an aggregate operation such as \verb|reduce|
on a distributed collection).

Let us revisit our previous example and kick off materialization to
investigate data flow and location of persistence:

\begin{lstlisting}
val adults: SiloRef[List[Person]] = ...
val vehicles: SiloRef[List[Vehicle]] = ...
val owners: Future[List[(Person,Vehicle)]] =
  adults.flatMap(...).send()
\end{lstlisting}
\noindent
For illustration we use the informal notation \verb|@m| to denote the location
of a value. We assume that the silo references are at machine \verb|m1| but the 
actual data is distributed over \verb|m2| and \verb|m3|:

\begin{lstlisting}
adults   @ m1 --> Silo[List[Person]]   @ m2
vehicles @ m1 --> Silo[List[Vehicle]]  @ m3
\end{lstlisting}

\noindent
To create \verb|owners|, we must combine data hosted at \verb|m2| with data
hosted at \verb|m3|. First, \verb|adults.flatMap| transfers its spore, i.e., the
silo reference \verb|vehicles| and the closure \verb|persons => ...|, to
\verb|m2| hosting the referenced silo of grown-up \verb|Person| records. Next,
\verb|localVehicles.map| transfers its spore, i.e., the collection of adults
\verb|persons| and the closure \verb|vehicles => ...|, to \verb|m3| hosting the
referenced silo of \verb|Vehicle| records. Now, at \verb|m3|, we have all
required information: the adults \verb|persons|, the vehicles \verb|vehicles|,
and the necessary computations to combine corresponding records, resulting in a
new (anonymous) silo reference at \verb|m2| referencing the new silo of
\verb|(Person,Vehicle)| records hosted at \verb|m3|. This new silo
reference at \verb|m2| is used to transfer its referenced data from \verb|m3| to
\verb|m2|, the origin of the \verb|flatMap|, and, eventually, to \verb|m1| where
the materialization has been kicked off.

To reduce the amount of data that is transferred, the implementation in fact
leverages {\em silo reference proxies}, delegating to the actual data:

\begin{lstlisting}
adults   @ m1                --> Silo[List[Person]]            @ m2
vehicles @ m1                --> Silo[List[Vehicle]]           @ m3
owners   @ m1 --> proxy @ m2 --> Silo[List[(Person,Vehicle)]]  @ m3
\end{lstlisting}
\noindent
The anonymous silo reference created at \verb|m2| will not be ruled out but
function as proxy delegating requests re \verb|owners| to the data hosted at
\verb|m3|. That is, the actual list of persons owing a vehicle is retrieved from
\verb|m3|. This alleviates network communication and scales up transformation
execution.

\paragraph{cache}%
%
\texttt{def cache(): Future[Unit]} \newline
%
The performance of typical data analytics jobs can be increased dramatically by
caching large data sets in memory~\cite{Spark}. To do this, the silo, containing
the computed data set, needs to be materialized. The only way, that we have
shown so far, to materialize a silo is using the \verb|send| primitive. However,
\verb|send| additionally transfers the contents of a silo to the requesting
node--too much if a large remote data set should merely be cached in memory
remotely. Therefore, an additional primitive called \verb|cache| is provided,
which forces the remote materialization of the given \verb|SiloRef|, returning
\verb|Future[Unit]|.

Given the running example so far, we can add another lineage branching off of
\verb|adults| by sorting the list of \verb|Person| records by age, producing a
greeting \verb|String|, and then ``kicking off'' remote computation by calling
\verb|cache|, rather than \verb|send|, in order to stash the result in remote
memory (graphically shown in Figure~\ref{fig:bigger-dag}, part 3 and 4):

\begin{lstlisting}
val sorted = adults.map(spore { ps => ps.sortWith(p => p.age) })
val labels = sorted.map(spore { ps => ps.map(p => "Hi " + p.name) })
labels.cache()
\end{lstlisting}
\noindent
Assuming we would also cache \verb|owners| from the previous example, the
resulting lineage graph would look as illustrated in
Figure~\ref{fig:bigger-dag}. Note that \verb|vehicles| is not a regular parent
in the lineage of \verb|owners|; it is an indirect input used to compute
\verb|owners| by virtue of being {\em captured} by the spore used to compute
\verb|owners|.

% The \verb|cache| method can be provided for convenience. It invokes
% \verb|send| not directly on the given SiloRef (which would transfer all data
% of the silo to the current node); instead, it first uses \verb|flatMap| to
% create a new silo that will be completed with the trivial value (e.g., a
% Boolean constant) of the \verb|DoneSiloRef| singleton object. Essentially,
% invoking \verb|send| on this trivial SiloRef causes the resulting future to be
% completed as soon as \verb|this| SiloRef has been materialized in main memory.

\subsubsection{Creating Silos}
\label{sec:creating-silos}

Besides a type definition for \verb|SiloRef[T]|, our framework also provides a
companion singleton object (Scala's form of modules). The singleton object
provides factory methods for obtaining silo references referring to silos populated
with some initial data:\footnote{For clarity, only method signatures are shown.}

\begin{lstlisting}
object SiloRef {
  def fromTextFile(host: Host)(file: File): SiloRef[List[String]]
  def fromFun[T](host: Host)(s: Spore[Unit, T]): SiloRef[T]
  def fromLineage[T](host: Host)(s: SiloRef[T]): SiloRef[T]
}
\end{lstlisting}
\noindent
Each of the factory methods has a \verb|host| parameter that specifies the
target host (address/port) on which to create the silo. Note that the
\verb|fromFun| method takes a spore closure as an argument to make sure it can
be serialized and sent to \verb|host|. In each case, the returned
\verb|SiloRef[T]| contains its \verb|host| as well as a
host-unique identifier. The \verb|fromLineage| method is
particularly interesting as it creates a copy of a previously existing silo
based on the lineage of a silo reference \verb|s|. Note that only the silo
reference is necessary for this operation to successfully complete; the silo
originally hosting \verb|s| might already have failed.

\subsubsection{Type Polymorphism and Silos/SiloRefs}

An important property of silos is that they are polymorphic in the type of data
that they hold (\verb|Silo[T]|). Importantly, silos are not just polymorphic in
the {\em element type} of their data, but in the {\em type of their entire
dataset}. For example, a silo might contain a Red-Black tree with elements of
type \verb|Person| for some ADT \verb|Person|, ordered by one of the fields of
the \verb|Person| type. Another silo might contain a completely different
collection type, say, a linked list. This type polymorphism enables optimizing
silos according to their data access patterns. Given that different data types
may have specialized operations (e.g., a tree map could provide a range
projection), the key to enabling this type polymorphism is the fact that a
spore, sent to a silo, may apply arbitrary functions to the silo's data. Thus,
the \verb|SiloRef| API itself is not limited to providing just a fixed set of
built-in operations (in contrast to RDDs in Apache Spark, for example).

\subsubsection{Expressiveness}
\label{sec:expr}

\paragraph{Expressing \texttt{map}}

Leveraging the above-mentioned methods for creating silos, it is possible to
express \verb|map| in terms of \verb|flatMap|:

\begin{lstlisting}
def map[S](s: Spore[T, S]): SiloRef[S] =
  this.flatMap(spore {
    val localSpore = s
    (x: T) =>
      val res = localSpore(x)
      SiloRef.fromFun(currentHost)(spore {
        val localRes = res
        () => localRes
      })
  })
\end{lstlisting}

\noindent
This should come as no surprise, given that \verb|flatMap| is the monadic bind
operation on \verb|SiloRef|s, and \verb|SiloRef.fromFun| is the monadic return
operation. The reason why \verb|map| is provided as one of the main operations
of \verb|SiloRef|s is that direct uses of \verb|map| enable an important
optimization based on operation fusion~\cite{FlumeJava}.

\paragraph{Expressing \texttt{cache}}

The \verb|cache| operation can be expressed using \verb|flatMap| and
\verb|send|:

\begin{lstlisting}
def cache(): Future[Unit] = this.flatMap(spore {
 val localDoneSiloRef = DoneSiloRef
 res => localDoneSiloRef
}).send()

\end{lstlisting}
\noindent
Here, we first use \verb|flatMap| to create a new silo that will be completed
with the trivial value of the \verb|DoneSiloRef| singleton object (\eg
\verb|Unit|). Essentially, invoking \verb|send| on this trivial \verb|SiloRef| causes
the resulting future to be completed as soon as \verb|this| silo reference has been
materialized in memory. {\color{gray}The value type \verb|Unit| ensures no data is
actually sent of the wire--up to \verb|()|, the only value of type \verb|Unit|.}

\subsection{Fault Handling}
\label{sec:fault-handling}

The function passing model includes overloaded\todo{Still valid?} variants of
the primitives discussed so far which enable the definition of flexible fault
handling semantics. The main idea is to specify fault handlers for {\em
subgraphs of computation DAGs}. Our guiding principle is to make the definition
of the failure-free path through a computation DAG as simple as possible, while
still enabling the handling of faults at the fine-granular level of individual
silo references.

\paragraph{Defining fault handlers}

Fault handlers may be specified whenever the lineage of a silo reference is
extended. For this purpose, the introduced \verb|map| and \verb|flatMap|
primitives are overloaded. For example, consider our previous example, but
extended with a fault handler:

\begin{lstlisting}
val persons: SiloRef[List[Person]] = ...
val vehicles: SiloRef[List[Vehicle]] = ...
// copy of `vehicles` on different host `h`
val vehicles2 = SiloRef.fromFun(h)(spore {
  val localVehicles = vehicles
  () => localVehicles
})

val adults = persons.map(spore { ps =>
  ps.filter(p => p.age >= 18) })

// adults that own a vehicle
def computeOwners(v: SiloRef[List[Vehicle]]) = spore {
  val localVehicles = v
  (ps: List[Person]) => localVehicles.map(...)
}

val owners: SiloRef[List[(Person, Vehicle)]] =
  adults.flatMap(computeOwners(vehicles),
                 computeOwners(vehicles2))
\end{lstlisting}

Importantly, in the \verb|flatMap| call on the last line, in addition to
\verb|computeOwners(vehicles)|, the regular spore argument of \verb|flatMap|,
\verb|computeOwners(vehicles2)| is passed as an additional argument. The second
argument registers a {\em failure handler} for the subgraph of the computation
DAG starting at \verb|adults|. This means that if during the execution of
\verb|computeOwners(vehicles)| it is detected that the \verb|vehicles| silo
reference has failed, it is checked whether the \verb|SiloRef| that the
higher-order combinator was invoked on (in this case, \verb|adults|) has a
failure handler registered. In that case, the failure handler is used as an
alternative spore to compute the result of \verb|adults.flatMap(..)|. In this
example, we specified \verb|computeOwners(vehicles2)| as the failure handler;
thus, in case \verb|vehicles| has failed, the computation is retried using
\verb|vehicles2| instead.

\section{Higher-Order Operations}
\label{sec:higher-order-operations}

\todo{In order to subsume the subsequent subsections, I'd rename this section
header to "Higher-Order Patterns" or just "High-Order"}

The introduced primitives enable expressing surprisingly intricate computational
patterns.

Higher-order operations such as variants of \verb|map|, \verb|reduce|, and
\verb|join|, operating on collections of data partitions, distributed across a
set of hosts, are required when implementing abstractions like Apache Spark's
distributed collections~\cite{Spark}. Section~\ref{sec:dist-coll} demonstrates
the implementation of some such operations in terms of silos.

In addition, even more patterns are possible thanks to the decentralized nature
of our programming model, which removes the limitations of master/worker host
configurations. Section~\ref{sec:decentral} shows examples of peer-to-peer
patterns that are still fault-tolerant.

% \verb|union|, \verb|groupByKey|, or \verb|join|

\subsection{Higher-Order Operations}
\label{sec:dist-coll}

% \paragraph{union}
%
% The union of two unordered collections stored in two different silos can be
% expressed directly using the above \verb|flatMap| primitive.

\paragraph{join}

Suppose we are given two silos with the following types:

\begin{lstlisting}
val silo1: SiloRef[List[A]]
val silo2: SiloRef[List[B]]
\end{lstlisting}
\noindent
as well as two hash functions computing hashes (of type \verb|K|) for elements
of type \verb|A| and type \verb|B|, respectively:

\begin{lstlisting}
val hashA: A => K = ...
val hashB: B => K = ...
\end{lstlisting}
\noindent
The goal is to compute the hash-join of \verb|silo1| and \verb|silo2| using a
higher-order operation \verb|hashJoin|:

\begin{lstlisting}
def hashJoin[A, B, K](s1: SiloRef[List[A]],
                      s2: SiloRef[List[B]],
                      f: A => K,
                      g: B => K)
  : SiloRef[List[(K, (A, B))]] = ???
\end{lstlisting}
\noindent
To implement \verb|hashJoin| in terms of silos, the types of the two silos first
have to be made equal, through initial \verb|map| invocations:

\begin{lstlisting}
val s12: SiloRef[List[(K, Option[A], Option[B])]] =
  s1.map(spore { l1 => l1.map(x => (f(x), Some(x), None)) })
val s22: SiloRef[List[(K, Option[A], Option[B])]] =
  s2.map(spore { l2 => l2.map(x => (g(x), None, Some(x))) })
\end{lstlisting}
\noindent
Then, we can use \verb|flatMap| to create a new silo which contains the elements
of both silo \verb|s12| and silo \verb|s22|:

\begin{lstlisting}
val combined = s12.flatMap(spore {
  val localS22 = s22
  (triples1: List[(K, Option[A], Option[B])]) =>
    s22.map(spore {
      val localTriples1 = triples1
      (triples2: List[(K, Option[A], Option[B])]) =>
        localTriples1 ++ triples2
    })
})
\end{lstlisting}
\noindent
The combined silo contains triples of type \verb|(K, Option[A], Option[B])|.
Using an additional \verb|map|, the collection can be sorted by key, and
adjacent triples be combined, yielding a \texttt{SiloRef[List[(K, (A, B))]]} as
required.

\paragraph{Partitioning and groupByKey}

A \verb|groupByKey| operation on a group of silos containing collections needs
to create multiple result silos, on each node, with ranges of keys supposed to
be shipped to destination hosts. These destination hosts are determined using a
partitioning function. Our goal, concretely:

\begin{lstlisting}
val groupedSilos = groupByKey(silos)
\end{lstlisting}
\noindent
Furthermore, we assume that \verb|silos.size| $= N$ where $N$ is the number of
hosts, with hosts $h_1$, $h_2$, etc. We assume each silo contains an unordered
collection of key-value pairs (a multi-map). Then, \verb|groupByKey| can be
implemented as follows:

\begin{itemize}
  \item Each host $h_i$ applies a {\em partitioning function} (example:
    \texttt{hash(key) mod N}) to the key-value pairs in its silo, yielding $N$
    (local) silos.

  \item Using \verb|flatMap|, each pair of silos containing keys of the same
    range can be combined and materialized on the right destination host.
\end{itemize}

Using just the primitives introduced earlier, applying the partitioning function
in this way would require $N$ \verb|map| invocations per silo. Thus, the
performance of \verb|groupByKey| could be increased significantly using a
specialized combinator, say, ``mapPartition'' that would apply a given
partitioning function to each key-value pair, simultaneously populating $N$
silos (where $N$ is the number of ``buckets'' of the partitioning function).

\subsection{Peer-to-Peer Patterns}
\label{sec:decentral}

\subsubsection{Essence}

So far, our examples have focused on master-worker topologies that underly
models like Apache Spark--\ie a master node specifies identical DAGs of
computation for all worker nodes to follow.

The function passing model, however, is not limited to these sorts of
topologies. It is indeed possible to develop decentralized, peer-to-peer
topologies on top of the function passing model. For example, a single compute
node may host silos, that are remotely referenced by remote SiloRefs, as well as
SiloRefs remotely referencing silos on other compute nodes.

Further, as we show in the following example, it's also possible for multiple
clients to build completely different DAGs of computation starting off of some
source silo. In effect, this enables datasets to be shared--they exist once in
memory on some node, but can be used and transformed in different ways by
different clients.

Consider the following example. We start by populating an initial silo
representing a dataset of \verb|Vehicle| objects on
\verb|Host("lmpsrv1.scala-lang.org", 9999)|.

\begin{lstlisting}
val lmpsrv1 = Host("lmpsrv1.scala-lang.org", 9999)

// client #1
// populate initial silo
val vehicles: SiloRef[List[Vehicle]] =
  Silo.fromTextFile(lmpsrv1)("hdfs://...")

val silo2 = vehicles.map(spore {
  (vs: List[Vehicle]) =>
    // extract US state from license plate string, e.g, "FL329098"
    vs.map(v => (v.licensePlate.take(2), v)).toMap
})
val vehiclesPerState = silo2.send()

// client #2
// get ref for silo that is being materialized due to client #1
val vehicles: SiloRef[List[Vehicle]] =
  Silo.fromTextFile(lmpsrv1)("hdfs://...")

val silo2 = vehicles.map(spore {
  // list all vehicles manufactured since 2013
  (vs: List[Vehicle]) => vs.filter(v => v.yearManufactured >= 2013)
})
val vehiclesSince2013 = silo2.send()
\end{lstlisting}

Here, client \#1 would like to perform some sort of computation based on the
states that vehicles are registered in. Another client, client \#2 would also
like to access this dataset. To do so, one must simply once again invoke
\verb|fromTextFile| on the same host, \verb|Host("lmpsrv1.scala-lang.org",9999)|
to obtain a SiloRef that points to a corresponding silo that is already or soon
to be materialized. From here, client \#2 is able to build an entirely different
DAG of computations, for instance in this example, filtering the original
\verb|vehicle| dataset to obtain only vehicles manufactured since 2013.

\subsubsection{Decentralized Fault-Handling}

Another peer-to-peer pattern possible in the function passing model is
decentralized fault handling. One may specify strategies to transfer computation
to other nodes in the event of failure.

Consider the following example: an aggregation should be performed as soon as
two silos \verb|vehicles| and \verb|persons| have been materialized. The
aggregation result is then combined with a silo \verb|info| on some host
different from the local host. The final result is written to a distributed file
system:

\vspace{-0.8mm}
\begin{lstlisting}
object Utils {
  def aggregate(vs: SiloRef[List[Vehicle]],
                ps: SiloRef[List[Person]]): SiloRef[String] = ...
  def write(result: String, fileName: String): Unit = ...
}
val vehicles: SiloRef[List[Vehicle]] = ...
val persons:  SiloRef[List[Person]]  = ...
val info:     SiloRef[Info]          = ...
val fileName: String                 = "hdfs://..."
val done    = info.flatMap(spore {
  val localVehicles = vehicles
  val localPersons  = persons
  (localInfo: Info) =>
    aggregate(localVehicles, localPersons).map(spore {
      val in = localInfo
      res => combine(res, in)
    })
}).map(spore {
  val captured = fileName
  combined => Utils.write(combined, captured)
})
done.cache() // force computation
\end{lstlisting}
\noindent
This program does not tolerate failures of the host of \verb|info|: if it fails
before the computation is complete, the result is never written to the file.

We can overcome this using fault handlers. It is possible to introduce another
backup host which takes over in case the host of \verb|info| (which is the same
as the host of \verb|done|) fails at any point. Let's try the above computation
again, this time using fault handlers to transfer the computation to a backup
node in the event of a failure:

\begin{lstlisting}
val doCombine = spore {
  val localVehicles = vehicles
  val localPersons  = persons
  (localInfo: Info) => aggregate(localVehicles, localPersons).map(
    spore {
      val in = localInfo
      res => combine(res, in)
    })
}
val doWrite = spore {
  val captured = fileName
  combined => Utils.write(combined, captured)
}
val done      = info.flatMap(doCombine).map(doWrite)
val backup    = SiloRef.fromFun(hostb)(spore { () => true })
val recovered = backup.flatMap(
  spore {
    val localDone = done
    x => localDone
  },
  spore { // fault handler
    val localInfo      = info
    val localDoCombine = doCombine
    val localDoWrite   = doWrite
    val localHostb     = hostb
    x =>
      // fromLineage makes sure, we re-run on hostb, rather than
      // the host of info. That is, we just duplicate the lineage.
      val restoredInfo = SiloRef.fromLineage(localHostb)(localInfo)
      restoredInfo.flatMap(localDoCombine).map(localDoWrite)
  }
)
done.cache()      // force computation on host of local
recovered.cache() // force computation on backup host
\end{lstlisting}
\noindent

First, the local variables \verb|doCombine| and \verb|doWrite| refer to the
verbatim spores passed to \verb|flatMap| and \verb|map| above. Second,
\verb|backup| is a dummy silo on a backup host \verb|hostb|. It is used to send
a spore to the backup host in a way that allows it to detect whether the host of
\verb|done|/\verb|info| has failed. The fault handling is done by calling
\verb|flatMap| on \verb|backup|, passing (a) a spore for the non-failure case
and (b) a spore for the failure case. The spore for the non-failure case simply
returns the \verb|done| SiloRef. Importantly, this enables \verb|hostb| to
detect failures of the host of \verb|done|. Upon detecting such a failure,
\verb|backup.flatMap| applies the spore for the failure case. In this case, the
lineage of the captured \verb|info| SiloRef is used to restore its original
contents in a new silo created on the backup host \verb|hostb|. Its SiloRef is
then used to retry the original computation.

% To illustrate the decentralized nature of our model, consider the following
% example: the local host aggregates some data as soon as two silos
% \verb|vehicles| and \verb|persons| have been materialized. The aggregation
% result is then combined with a silo \verb|info| on local host. The final
% result is written to a distributed file system:
%
% \begin{lstlisting}
% object Utils {
%   def aggregate(vs: SiloRef[List[Vehicle]],
%                 ps: SiloRef[List[Person]]): SiloRef[String] = ...
%   def write(result: String, fileName: String): Unit = ...
% }
% val vehicles: SiloRef[List[Vehicle]] = ...
% val persons:  SiloRef[List[Person]]  = ...
% val info:     SiloRef[Info]          = ...
% val fileName: String                 = "hdfs://..."
% val done    = info.flatMap(spore {
%   val localVehicles = vehicles
%   val localPersons  = persons
%   (localInfo: Info) =>
%     aggregate(localVehicles, localPersons).map(spore {
%       val in = localInfo
%       res => combine(res, in)
%     })
% }).map(spore {
%   val captured = fileName
%   combined => Utils.write(combined, captured)
% })
% done.cache() // force computation
% \end{lstlisting}
% \noindent
% This program does not tolerate failures of the local host: if it fails before
% the computation is complete, the result is never written to the file. Using
% fault handlers, though, it is easy to introduce a backup host that takes over
% in case the local host fails at any point:
%
% \begin{lstlisting}
% val doCombine = spore {
%   val localVehicles = vehicles
%   val localPersons  = persons
%   (localInfo: Info) =>
%     aggregate(localVehicles, localPersons).map(spore {
%       val in = localInfo
%       res => combine(res, in)
%     })
% }
% val doWrite = spore {
%   val captured = fileName
%   combined => Utils.write(combined, captured)
% }
% val done      = info.flatMap(doCombine).map(doWrite)
% val backup    = SiloRef.fromFun(hostb)(spore { () => true })
% val recovered = backup.flatMap(
%   spore {
%     val localDone = done
%     x => localDone
%   },
%   spore { // fault handler
%     val localInfo      = info
%     val localDoCombine = doCombine
%     val localDoWrite   = doWrite
%     val localHostb     = hostb
%     x =>
%       val restoredInfo = SiloRef.fromLineage(localHostb)(localInfo)
%       restoredInfo.flatMap(localDoCombine).map(localDoWrite)
%   }
% )
% done.cache()      // force computation on local host
% recovered.cache() // force computation on backup host
% \end{lstlisting}
% \noindent First, the local variables \verb|doCombine| and \verb|doWrite| refer
% to the verbatim spores passed to \verb|flatMap| and \verb|map| above. Second,
% \verb|backup| is a dummy silo on a backup host \verb|hostb|. It is used to
% send a spore to the backup host in a way that allows it to detect whether the
% original host has failed. The fault handling is done by calling \verb|flatMap|
% on \verb|backup|, passing (a) a spore for the non-failure case  (b) a spore
% for the failure case. The spore for the non-failure case simply returns the
% \verb|done| SiloRef. The spore for the failure case is applied whenever the
% value of the \verb|done| SiloRef could not be obtained. In this case, the
% lineage of the captured \verb|info| SiloRef is used to restore its original
% contents in a new silo created on the backup host \verb|hostb|. Its SiloRef is
% then used to retry the original computation. In case the original host failed
% only after the materialization of \verb|vehicles| and \verb|persons|
% completed, their cached data is reused.

% // concat
% def concat(silo1: SiloRef[List[A]],
%            silo2: SiloRef[List[A]]) =
%   silo1.flatMap(spore {
%     val localSilo2 = silo2
%     list1 =>
%       localSilo2.map(spore {
%         val localList1 = list1
%         list2 => localList1 ++ list2
%       })
%   })

% Operations on distributed collections\todo{somehow mention Spark here so it's
% clear what a dist coll is.}~such as \verb|union|, \verb|groupByKey|, or
% \verb|join|, involve multiple data sets, possibly located on different nodes.
% In the following we explain how such operations can be expressed using the
% introduced primitives.\todo{need a more compelling into for PL people}

\section{Formalization}
\label{sec:formalization}
\input{formalization}

\section{Implementation}
\label{sec:implementation}

The presented programming model has been fully implemented in Scala, a
functional programming language that runs on both JVMs and JavaScript runtimes.
The function passing model is compiled and run using Scala 2.11.8, and considers
only the JVM backend for now. Our implementation, which has been published as an
open-source project,\footnote{URL withheld for blind review.} builds on two main
Scala extensions:

\begin{itemize}

  \item First, Pickling~\cite{Pickling},
    \footnote{\url{https://github.com/scala/pickling}} a type-safe and
    performant serialization library with an accompanying, optional macro
    extension that is focused on distributed programming. It is used for all
    serialization tasks. Our function passing implementation benefits from the
    maturity of Pickling, which supports pickling/unpickling a wide range of
    Scala type constructors. Pickling has evolved from a research prototype to a
    production-ready serialization framework that is now in widespread
    commercial use.

  \item Second, the programming model makes extensive use of spores,
    closure-like objects with explicit, typed environments. While~\cite{Spores}
    has reported an an empirical evaluation of spores, our presented programming
    model and implementation turned out to be an extensive validation of spores
    in the context of distributed programming. In addition, our implementation
    required a thorough refinement of the way spores are pickled.

\end{itemize}

So far, we have used our implementation to build a small Apache Spark-like
distributed collections abstraction, a simple version of the
MBrace framework, and example data analytics applications, such as
word count and group-by-join pipelines. We give an introduction and step through
some of these example applications in section~\ref{sec:examples}.

% Our prototype has also served as an experimentation platform for type-based
% optimizations, which we present in more detail below.

% Further, we maintain a growing collection of these toy distributed frameworks
% (built on top of function passing) and applications on top of them (such as
% data analytics) at \url{http://lampwww.epfl.ch/~hmiller/f-p/}.\footnote{Review
% note: this site is publicly accessible, has no tracking capabilities, and is
% linked to in talks and from other various webpages meaning it already gets
% public traffic. One may visit it without identity concerns.}

\subsection{Serialization in the presence of existential quantification}

Initially, to serialize most message types exchanged by the network
communication layer, runtime-based unpicklers had to be used (meaning unpickling
code discovering the structure of a type through introspection at runtime). A
major disadvantage of runtime-based unpickling is its significant impact on
performance. The reason for its initial necessity was that message types are
typically generic, but the generic type arguments are {\em
existentially-quantified type variables} on the receiver's side. For example,
the lineage of a SiloRef may contain instances of a type Mapped. This generic
type has four type parameters. The receiver of a freshly unpickled Mapped
instance typically uses a pattern match:

\begin{lstlisting}
case mapped: Mapped[u, t, v, s] =>
\end{lstlisting}

The type arguments \verb|u|, \verb|t|, \verb|v|, and \verb|s| are {\em type
variables}. While unknown, the static type of \verb|mapped| is still useful for
type-safety:

\begin{lstlisting}
val newSilo = new LocalSilo[v, s](mapped.fun(value))
\end{lstlisting}

However, it is impossible to generate type-specific code to unpickle a type like
\verb|Mapped[u, t, v, s]|. As a solution to this problem we propose what we call
``self-describing'' pickles. Basically, the idea is to augment the serialized
representation with additional information about how to unpickle. The key is to
capture the type-specific pickler and unpickler when the fully-concrete type of
a \verb|Mapped| instance is known:

\begin{lstlisting}
def doPickle[T](msg: T)
  (implicit pickler: Pickler[T],
          unpickler: Unpickler[T]): Array[Byte] = ...
\end{lstlisting}

Essentially, this means when \verb|doPickle| is called with a concrete type
\verb|T|, say:\footnote{Note that the type arguments are inferred by the Scala
compiler; they are only shown for clarity.}

\begin{lstlisting}
doPickle[Mapped[Int, List[Int], String, List[String]]](mapped)
\end{lstlisting}

\noindent not only a type-specific implicit pickler (a type class instance) is
looked up, but also a type-specific implicit unpickler. The \verb|doPickle|
method can then build a self-describing pickle as follows. First, the actual
message is pickled using the pickler, yielding a byte array. Then, an instance
of the following simple record-like class is created:

\begin{lstlisting}
case class SelfDescribing(blob: Array[Byte],
                          unpicklerClassName: String)
\end{lstlisting}

Besides the just produced byte array, it contains the class name of the type-
specific unpickler. This enables, using this fully type-specific unpickler, even
when the message type to be unpickled is only partially known. All that is
required is an unpickler for type \verb|SelfDescribing|. First, it reads the
byte array and class name from the pickle. Second, it instantiates the type-
specific unpickler reflectively using the class name. (Note that this is
possible on both the JVM as well as on JavaScript runtimes using Scala's current
JavaScript backend.) Finally, the unpickler is used to unpickle the byte array.
In conclusion, this approach ensures (a) that a type that is pickleable using a
type-specific pickler is guaranteed to be unpickleable by the receiver of the
pickled SelfDescribing instance, and (b) that unpickling is as efficient as
pickling, thanks to using type-specific unpicklers.

% \subsection{Type-based optimization of serialization}

% \begin{figure}[t!]
% \centering\includegraphics[width=\columnwidth]{multipleJVM.pdf}
% \caption{Impact of Static Types on Performance, End-to-End Application (\texttt{groupBy} + \texttt{join}).}
% \label{fig:multiple-jvm}
% \end{figure}
%
% We have used our implementation to measure the impact of type-specific,
% compile-time-generated serializers (see above) on end-to-end application
% performance. In our benchmark application, a group of 4 silos is distributed
% across 4 different nodes/JVMs. Each silo is populated with a collection of
% ``person'' records. The application first transforms each silo using
% \emph{map}, and then using \emph{groupBy} and \emph{join}. For the benchmark
% we measure the running time for a varying number of records.
%
% We ran our experiments on a 2.3 GHz Intel Core i7 with 16 GB RAM under Mac OS
% X 10.9.5 using Java HotSpot Server 1.8.0-b132. For each input size we report
% the median of 7 runs. Figure~\ref{fig:multiple-jvm} shows the results.
% Interestingly, for an input size of 100,000 records, the use of type-specific
% serializers resulted in an overall speedup of about 48\% with respect to the
% same system using runtime-based serializers.

\section{Examples}
\label{sec:examples}

To show that the function passing model is able to serve as a substrate upon
which to build different sorts of data-centric distributed frameworks, we
provide two miniaturized example systems built on top of F-P that are inspired
by popular big data frameworks; Apache Spark's RDD (Resilient Distributed
Datasets), and MBrace. Using each example system, we've implemented several
example applications, excerpts of some of which are explained below.

Apache Spark's RDDs provide a set of operations to execute parallel operations
on distributed data. MBrace extends F\#’s asynchronous workflows, a way to
declare asynchronous tasks, to distribute computation in the cloud.

Our simplified RDD implementation lets you use data structures distributed
inside Silos using the function passing model. We've implemented some of the
operations of Apache Spark’s RDD such as \verb|map|, \verb|reduce|,
\verb|groupBy| and \verb|join| in terms of the function passing model's
primitives. Methods on RDDs like \verb|flatMap| or \verb|filter| that don’t need
to communicate across Silos are implemented with the function passing model’s
\verb|map| using the corresponding method on the underlying Scala’s collections,
while \verb|join| is implemented using the function passing model’s
\verb|flatMap|. Below, we use it to show a simple application associating the
length of all words in two documents in a \verb|Map| to a set of words with the
corresponding length.

\begin{lstlisting}
val content: RDD[String, List[String]] = ...
val lorem: RDD[String, List[String]] = ...

val contentWord = content.flatMap(line => {
  line.split(' ').toList
}).map(word => (word.length, word))

val loremWord = lorem.flatMap(line => {
  line.split(' ').toList
}).map(word => (word.length, word))

val res: Map[Int, Set[String]] =
  contentWord.join[Set, Map](loremWord).collectMap()
\end{lstlisting}

In this example, the closure inside the RDD's \verb|flatMap| method called on
each \verb|contentWord| and \verb|loremWord| splits each line as a list of
words, and flattens everything as a single list. Each word is then mapped to a
tuple containing its length and the word. Finally, we do an inner join, which in
turn associates each length to the set of words of the same length, removing
duplicate words in the process. Finally, we collect the final result in a
\verb|Map| using the \verb|collectMap| method on RDDs. Several other more
detailed example programs using Apache Spark on the function passing model are
available on GitHub~\footnote{URL withheld for blind review.}

In the context of MBrace, we also implemented the K-Means Clustering example
available on the MBrace's website
\footnote{\url{http://mbrace.io/starterkit/HandsOnTutorial/examples/200-kmeans-clustering-example.html}},
an excerpt of which is shown below. Our implementation of a distributed K-Means
using the function passing model is an almost identical port of the MBrace’s
version written in F\#. K-Means is an algorithm to categorize data points across
$K$ different clusters. It starts with the centroids of the $K$ clusters.

\begin{lstlisting}
def kMeansIterate(
  partitionedPoints: Seq[SiloRef[Array[Point]]],
  centroids: Array[Point],
  iteration: Int): Array[Point] = {
  // Running iteration...
  val clusterParts =
    partitionedPoints.map(silo => { silo.map(spore {
      val lCentroids = centroids
      points => kmeansLocal(points, lCentroids)
    }).send()
  })

  val newCentroids =
    Await.result(Future.sequence(clusterParts).map(seq => {
      seq
        .reduce(_ ++ _)
        .groupBy(_._1)
        .toSeq
        .sortBy(_._1)
        .map(_._2)
        .map(clp => clp.map(_._2).toArray.unzip)
        .map({ case (ns, points) => ns.sum -> sumPoints(points) })
        .map({ case (n, sum) => divPoint(sum, n) })
    }), 10.seconds).toArray

  val diff = newCentroids
    .zip(centroids)
    .map({ case (p1, p2) => dist(p1, p2) })
    .max

  // check if converged, else iterate again
  if (diff < epsilon) {
    newCentroids
  } else {
    kMeansIterate(partitionedPoints, newCentroids, iteration + 1)
  }
}
\end{lstlisting}
The algorithm proceeds with two steps. It first assigns data points to the
closest cluster. Then it assigns to each cluster a new centroid by computing the
mean of the points assigned to the clusters. It stops when the centroids stop
changing; if this convergence condition hasn't been met, the algorithm is called
recursively with the updated set of centroids. In the distributed version of
K-Means, we start with a master node that partitions the points into silos. At
each iteration, \verb|map| is called on the SiloRef which results in a spore
(function) being applied to the data within the corresponding silo. The spore
captures the current iterations' centroids and uses them to compute the new
cluster for its local set of points (using the \verb|kmeansLocal| function). The
results are then sent back to the master node to compute the new centroids, and
to verify the algorithm's convergence condition.

% \section{Evaluation}
% \label{sec:evaluation}

% We provide a small evaluation of our implementation to demonstrate the
% practical benefit of some of the design points of our model.

\section{Related Work}
\label{sec:related-work}

Alice ML~\cite{AliceML} is an extension of Standard ML which adds a number of
important features for distributed programming such as futures and proxies. The
design leading up to the function passing model has incorporated many similar
ideas, such as {type-safe}, generic and platform-independent pickling. In Alice,
functions intend to be mobile. Only those functions which capture (either
directly or indirectly) local resources remain stationary. In the case of
functions that must remain stationary, it is possible to send proxies, mobile
wrappers for functions. Sending a proxy will not transfer the wrapped function;
instead, when a proxy function is applied, the call is forwarded by the system
to the original site as a remote invocation (pickling arguments and result
appropriately). In F-P, however, functions are not wrapped in proxies but sent
directly. Thus, calling a received function will not lead to remote invocations.

Cloud Haskell~\cite{CloudHaskell} leverages guaranteed-serializable, static
closures for a message-passing communication model inspired by Erlang. In
contrast, in our model spores are sent between passive, persistent silos. In
contrast, in our model spores are sent between passive, persistent silos.
Moreover, the coordination of concurrent activity is based on futures, instead
of message passing. Closures and continuations in Termite
Scheme~\cite{TermiteScheme} are always serializable; references to
non-serializable objects (like open files) are automatically wrapped in
processes that are serialized as their process ID. Similar to Cloud Haskell,
Termite is inspired by Erlang. In contrast to Termite, the function passing
model is statically typed, enabling advanced type-based optimizations. In
non-process-oriented models, parallel closures~\cite{ParallelClosures} and
RiverTrail~\cite{RiverTrail} address important safety issues of closures in a
concurrent setting. However, RiverTrail currently does not support capturing
variables in closures, which is critical for the flatMap combinator in the
function passing model. In contrast to parallel closures, spores do not require
a type system extension in Scala.

Acute ML~\cite{AcuteML} is a dialect of ML which proposes numerous primitives
for distributed programming, such as type-safe serialization, dynamic linking
and rebinding, and versioning. The function passing model, in contrast, is based
on spores, which ship with their serialized environment or they fail to compile,
obviating the need for dynamic rebinding. HashCaml~\cite{DistOCaml} is a
practical evolution of Acute ML's ideas in the form of an extension to the OCaml
bytecode compiler, which focuses on type-safe serialization and providing
globally meaningful type names. In contrast, function passing is merely a
programming model, which does not require extensions to the Scala compiler.
% compiler, which focuses on transmitting safe functions to work on remote
% distributed data.

ML5~\cite{Tom7} provides mobile closures verified not to use resources not
present on machines where they are applied. This property is enforced
transitively (for all values reachable from captured values), which is stronger
than what plain spores provide. However, type constraints allow spores to
require properties not limited to mobility. Transitive properties are supported
either using type constraints based on type classes which enforce a transitive
property or by integrating with type systems that enforce transitive properties.
Unlike ML5, spores do not require a type system extension. Further, the function
passing model sits on top of these primitives to provide a full programming
model for distribution, which also integrates spores and type-safe pickling.

% However, type constraints allow spores to require properties not limited to
% mobility. Transitive prop- erties are supported either using type constraints
% based on type classes which enforce a transitive property or by integrating
% with type systems that enforce transitive properties. Unlike ML5, spores do
% not require a type system extension.

Systems like Apache Spark~\cite{Spark}, MapReduce~\cite{MapReduce}, and
Dryad~\cite{Dryad} are just that--distributed systems. The function passing
model is meant to act as more of a middleware to facilitate the design and
implementation of such systems, and as a result provides much finer-grained
control over details such as fault handling and network topology (\ie
peer-to-peer vs master/worker).

The Clojure programming language proposes agents~\cite{Clojure}--stationary
mutable data containers that users apply functions to in order to update an
agent's state. The function passing model, in contrast, proposes that data in
stationary containers be immutable, and that transformations by function
application form a persistent data structure. Further, Clojure's agents are
designed to manage state in a shared memory scenario, whereas the function
passing model is designed with remote references for a distributed scenario.

The function passing model is also related to the actor model of
concurrency~\cite{Actors}, which features multiple implementations in
Scala~\cite{ScalaActors,Akka,TypedActors}. Actors can serve as in-memory data
containers in a distributed system, like our silos. Unlike silos, actors
encapsulate behavior in addition to immutable or mutable values. While only some
actor implementations support mobile actors (none in Scala), mobile behavior in
the form of serializable closures is central to the function passing model.

% Session Types~\cite{SessionTypes}. AmbientTalk~\cite{AmbientTalk}. E
% Language~\cite{ELang}.

% Other clean slate language designs have been proposed to broadly address
% issues related to distributed programming. Thorn~\cite{Thorn} was designed
% with concurrency and the need to interact with remote services in mind. Though
% a main design goal of Thorn is to evolve scripts into typed programs via
% gradual typing. function passing on the other hand is not a language, but a
% programming model, which is designed to inject types into distributed programs
% from the get-go.

% function passing integrates a distributed, persistent data structure. Other
% prior work related to spores is discussed in~\cite{Spores}.

% Type-safe distributed programming in ML5~\cite{Tom7} introduces a notion of a
% typed context called a ``world'' and permits functions to be executed given a
% specific world.

\section{Future Work and Conclusion}
\label{sec:conclusion-future-work}

\subsection{Ongoing and Future Work}

In ongoing work we are exploring approaches for memory reclamation. The first
approach uses Java's WeakReferences to detect when a SiloRef is no longer
reachable from local GC roots. Upon detection the host of the corresponding silo
is notified to decrease the silo's reference count; the host's reference(s) to
the silo are nulled out when the reference count reaches zero. It is important
to note that this strategy requires notifying a silo's host whenever a SiloRef
to the silo reaches a new machine, to increase the silo's reference count. The
second approach leverages uniqueness types in Scala~\cite{Uniqueness}. Here,
SiloRefs are locally unique, and the programmer can explicitly declare a SiloRef
as unused; the type system ensures that such an ``unused'' SiloRef is not used
again subsequently. As in the first approach, upon marking a SiloRef as unused,
the corresponding silo's host is notified to decrease the silo's reference
count.

Other future work includes better understanding concerns of separate compilation
in order to evaluate whether our model could be of help in coordinating between
microservices.\footnote{Microservices are small, independent
(separately-compiled) services running on different machines which communicate
with each other to together make up a single and complex application. They are a
predominant trend in industry amongst rich and complicated web-based services.}

\subsection{Conclusion}

We have presented the function passing model, a new programming model and new
substrate or middleware upon which to build data-centric distributed systems.
This enables two important benefits for distributed system builders; since (a)
all computations are functional transformations on immutable data,
fault-tolerance is made simple by design, and (b) communication is made
well-typed by design, the function passing model attempts to more naturally
model the paradigm of data-centric programming by extending monadic programming
to the network. One insight of our model is that lineage-based fault recovery
mechanisms, used in widespread frameworks for distribution, can be modeled
elegantly in a functional way using persistent data structures. Our operational
semantics shows that this approach makes it even amenable to formal treatment.
We have also shown that the function passing model is able to express patterns
of computation richer than those supported by common ``big data'' frameworks
while maintaining fault-tolerance--such as decentralized peer-to-peer patterns
of communication. Finally, we have implemented our approach in and for Scala,
and have shown that it's possible to support different popular patterns of
distributed processing, such as  batch processing with Apache Spark's RDDs and
MBrace's cloud-based asynchronous tasks.

% We have presented the function passing model, a new programming model and new
% substrate or middleware upon which to build data-centric distributed systems.
%
% This enables two important benefits for distributed system builders; (a) since
% all computations are functional transformations on immutable data,
% fault-tolerance is made simple by design, and (b) communication is made
% well-typed by design, a common pain point for builders of distributed systems
% in Scala. Said another way, the function passing model attempts to more
% naturally model the paradigm of data-centric programming by extending monadic
% programming to the network.
%
% This new model is designed to make it easier to reason about One insight of
% our model is that lineage-based fault recovery mechanisms, used in widespread
% frameworks for distribution, can be modeled elegantly in a functional way
% using persistent data structures.
%
% Our operational semantics shows that this approach makes it even amenable to
% formal treatment.
%
% We have also shown that the function passing model is able to express patterns
% of computation richer than those supported by common ``big data'' frameworks
% while maintaining fault-tolerance--such as decentralized peer-to-peer patterns
% of communication.
%
% Finally, we have implemented our approach in and for Scala, as well as
% numerous applications on top. We've shown , and have discovered new ways to
% reconcile type-specific serializers with patterns of static typing common in
% distributed systems.

% We have presented the function passing model, a new programming model and
% principled substrate for building data-centric distributed systems. Built atop
% a foundation consisting of performant and type-safe serialization, and safe,
% serializable closures, we have shown that it's possible to build elegant
% fault-tolerant functional systems. One insight of our model is that
% lineage-based fault recovery mechanisms, used in widespread frameworks for
% distribution, can be modeled elegantly in a functional way using persistent
% data structures. Our operational semantics shows that this approach makes it
% even amenable to formal treatment. We have also shown that the function
% passing model is able to express patterns of computation richer than those
% supported by common ``big data'' frameworks while maintaining
% fault-tolerance--such as decentralized peer-to-peer patterns of communication.
% Finally, we have implemented our approach in and for Scala, as well as
% numerous applications on top, and have discovered new ways to reconcile
% type-specific serializers with patterns of static typing common in distributed
% systems.
%
% A great deal of future work remains. In the short-term, we aim to continue to
% build different sorts of distributed frameworks and applications atop the
% function passing model in an effort to work towards a production-ready
% implementation of our model for consumption by the Scala community at large.

% Adapt this model for streaming computation. This would take coming up with a
% solution to null out references in the {\em lineage}. This would make it
% possible to instantiate and populate new silos to handle incoming data.

% The most successful systems for ``big data'' processing have all adopted
% functional APIs. But the innards of these systems are often built atop
% imperative and weakly-typed stacks, which complicates the design and
% implementation of distributed system essentials like fault-tolerance. We
% present a new programming model we call  {\em function passing} designed to
% overcome many of these issues by providing a more principled substrate on
% which to build data-centric distributed systems. A key idea is to pass safe,
% well-typed serializable functions to immutable distributed data.  The function
% passing model itself can be thought of as a distributed persistent functional
% data structure, which stores in its nodes transformations to data rather than
% the distributed data itself.  Thus, the model simplifies failure recovery by
% design--data is recovered by replaying function applications atop immutable
% data loaded from stable storage. Lazy evaluation is also central to our model;
% by carefully incorporating laziness into our design (only at the point of
% initiating network communication), our model remains easy to reason about
% while remaining efficient in time and memory. We formalize our programming
% model in the form of a small-step operational semantics which includes a
% precise specification of the semantics of functional fault recovery. We
% implement our model in and for the Scala programming language, and provide a
% small evaluation of the efficiency of our implementation of the model.

% \section{Future Work}

% Adapt this model for streaming computation. This would take coming up with a
% solution to null out references in the {\em lineage}. This would make it
% possible to instantiate and populate new silos to handle incoming data.

% \section{Old}

% \subsection{Old Intro}

% % From the ivory tower, it may not be evident, % It might not seem this way
% from the ivory tower, % but

% While it might not seem that way at a glance, mainstream software development
% has become largely distributed. Two {\em styles} of distribution dominate the
% current landscape; (1) systems composed of microservices, % or designed to be
% software as a service (SaaS), and (2) systems for ``big data'' processing.
% Names like Netflix, SoundCloud, and Twitter have established their competitive
% offerings by way of one or both of these types of systems.

% Microservices are small, independent (separately-compiled) services running on
% different machines which communicate with each other to together make up a
% single and complex application. ``Big data''-style applications on the other
% hand are typically single applications whose data cannot fit into the memory
% of one machine alone. Such applications are typically singly-compiled, with
% their binaries distributed across a cluster of machines.

% Yet, software developers still fumble with low-level RPC frameworks.  % and
% sluggish serialization frameworks.  Mainstream programming languages have
% traditionally offered little support in this space.

% We design module systems, etc, for maximum reuse and productivity, we go as
% far as we can to demonstrate their power. But we often . Some of these
% concerns have been affectionately referred to as the ``awkward
% squad''~\cite{AwkwardSquad} by PL research, and they are concerns that are
% central to contemporary software development.

% Functional programming brings value to distributed systems builders. The
% success of popular Spark can be attributed to functional ideas.

% However, beyond Spark's interface, .

% We refer to the model as {\em function-passing}, and .

% We design a programming model in such a way that we can ``maximize'' static
% types, and with it enable type-specialized picklers (statically generated),
% typed closures, and type-specialized data structures.

% The motto is ``making type inference work for you when optimizing distributed
% systems'', or ``how type inference and existential types benefit distributed
% systems''.

% In existing systems, types are only in the user-facing API and help the user.
% However, all too often, the internals of a distributed system are largely
% untyped, in particular when operating on data types that are also shipped
% remotely. In our approach we now go ahead and make types work so that they
% benefit both users (helping catch common errors) \emph{and} distributed
% systems builders. Our approach leverages types to provide (1) type-specialized
% picklers, and (2) type-specialized collections/builders.

% The approach is novel, because noone talks about making sure the unpicklers
% and builders themselves are serializable, but that's actually fundamental to
% the whole design.

% \begin{itemize}

% \item We have to make the point that serialization is like a primitive in
% systems design, not some extra thing you figure out how to do later, like in
% typical PLs. so maybe one central point that we should try to make is that for
% distributed systems, serializability is like a key primitive, along with
% latency and other things.

% \item we want to prove that typed distributed systems are a good idea. right
% now a lot of systems for dist computing are fundamentally untyped or
% dynamically checked. those that are typed lose type information between
% machines.

% \item so we're trying to argue that types are good for system builders.

% \end{itemize}

% The fact that actually these benefits from types \emph{carry over to other
% models as well} (not just spark-like models).

% Generalization of MapReduce model. Low-level. Inversion of the actor model.
% Can represent different many models for distributed computing, e.g. Spark,
% Percolator (we probably can't validate this claim.)

% Illustrations to have:
% \begin{enumerate}
% \item nice figure
% \item organic evolving model on youtube
% \end{enumerate}

% \subsection{Evaluation}

% we could make the performance eval strong by saying, ok, we implemented real
% apps, so we have the real communication that needs to happen (and happens).
% and we now make this communication typed, plus we measure exactly the
% percentage of time that's spent in serialization for example. and then we
% could even interpolate the results, so we could say, ok, if some other dist
% system has a lower percentage spent in serialization, then the speed up would
% be like this.

% and then we could even measure that for real spark, and then interpolate and
% say, ok, in the ideal case, meaning using all of our design principles, we
% could speed up things like this, if one would re-architect spark to preserve
% types (it's a huge task, so out of scope).

% well, it would definitely be something that we can measure objectively. the
% only thing we have to guard against would be to say, adding picklers and
% builders does not otherwise \emph{degrade} performance, like, we'd have to
% prove that there's no cost, but \emph{just} the performance benefits. adding
% the classname of the unpickler adds a few bytes to each pickle, for example.

% we could even show that with babyspark, like have a version that uses java
% serialization, then a version that uses pickling without selfdescribing, then
% with runtime unpickling, and with all static. because we have those numbers
% for babyspark then the interpolation will be much more precise.

\newpage

\appendix
\label{sec:appendix}
\input{appendix}

\bibliography{bib}

\label{lastpage}

\end{document}

% end of JFP2egui.tex
